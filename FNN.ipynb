{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPXbb0FXxBztIOphRvvACEK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a1W6cEXXN7Ug","executionInfo":{"status":"ok","timestamp":1712554781272,"user_tz":0,"elapsed":17296,"user":{"displayName":"Márk Németh","userId":"08804061133513418249"}},"outputId":"a16add14-1d34-4b16-c988-21e595b1b7a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","1/1 [==============================] - 2s 2s/step - loss: 0.6714 - accuracy: 0.7500\n","Epoch 2/100\n","1/1 [==============================] - 0s 31ms/step - loss: 0.7553 - accuracy: 0.5000\n","Epoch 3/100\n","1/1 [==============================] - 0s 17ms/step - loss: 0.7453 - accuracy: 0.5000\n","Epoch 4/100\n","1/1 [==============================] - 0s 18ms/step - loss: 0.6433 - accuracy: 1.0000\n","Epoch 5/100\n","1/1 [==============================] - 0s 18ms/step - loss: 0.8783 - accuracy: 0.5000\n","Epoch 6/100\n","1/1 [==============================] - 0s 17ms/step - loss: 0.7462 - accuracy: 0.0000e+00\n","Epoch 7/100\n","1/1 [==============================] - 0s 23ms/step - loss: 0.8552 - accuracy: 0.5000\n","Epoch 8/100\n","1/1 [==============================] - 0s 19ms/step - loss: 0.7365 - accuracy: 0.7500\n","Epoch 9/100\n","1/1 [==============================] - 0s 19ms/step - loss: 0.8222 - accuracy: 0.7500\n","Epoch 10/100\n","1/1 [==============================] - 0s 16ms/step - loss: 0.6773 - accuracy: 0.7500\n","Epoch 11/100\n","1/1 [==============================] - 0s 25ms/step - loss: 0.7350 - accuracy: 0.5000\n","Epoch 12/100\n","1/1 [==============================] - 0s 15ms/step - loss: 0.7318 - accuracy: 0.7500\n","Epoch 13/100\n","1/1 [==============================] - 0s 17ms/step - loss: 0.7370 - accuracy: 0.7500\n","Epoch 14/100\n","1/1 [==============================] - 0s 14ms/step - loss: 0.7458 - accuracy: 0.5000\n","Epoch 15/100\n","1/1 [==============================] - 0s 13ms/step - loss: 0.7891 - accuracy: 0.2500\n","Epoch 16/100\n","1/1 [==============================] - 0s 16ms/step - loss: 0.7810 - accuracy: 0.5000\n","Epoch 17/100\n","1/1 [==============================] - 0s 13ms/step - loss: 0.7333 - accuracy: 0.5000\n","Epoch 18/100\n","1/1 [==============================] - 0s 16ms/step - loss: 0.8303 - accuracy: 0.5000\n","Epoch 19/100\n","1/1 [==============================] - 0s 15ms/step - loss: 0.8388 - accuracy: 0.5000\n","Epoch 20/100\n","1/1 [==============================] - 0s 16ms/step - loss: 0.6645 - accuracy: 0.7500\n","Epoch 21/100\n","1/1 [==============================] - 0s 17ms/step - loss: 0.7207 - accuracy: 1.0000\n","Epoch 22/100\n","1/1 [==============================] - 0s 13ms/step - loss: 0.7173 - accuracy: 0.5000\n","Epoch 23/100\n","1/1 [==============================] - 0s 15ms/step - loss: 0.7763 - accuracy: 0.5000\n","Epoch 24/100\n","1/1 [==============================] - 0s 16ms/step - loss: 0.8924 - accuracy: 0.2500\n","Epoch 25/100\n","1/1 [==============================] - 0s 16ms/step - loss: 0.8100 - accuracy: 0.2500\n","Epoch 26/100\n","1/1 [==============================] - 0s 24ms/step - loss: 0.6862 - accuracy: 0.7500\n","Epoch 27/100\n","1/1 [==============================] - 0s 35ms/step - loss: 0.6153 - accuracy: 1.0000\n","Epoch 28/100\n","1/1 [==============================] - 0s 20ms/step - loss: 0.7237 - accuracy: 0.5000\n","Epoch 29/100\n","1/1 [==============================] - 0s 18ms/step - loss: 0.7850 - accuracy: 0.5000\n","Epoch 30/100\n","1/1 [==============================] - 0s 27ms/step - loss: 0.7577 - accuracy: 0.5000\n","Epoch 31/100\n","1/1 [==============================] - 0s 24ms/step - loss: 0.7695 - accuracy: 0.5000\n","Epoch 32/100\n","1/1 [==============================] - 0s 13ms/step - loss: 0.6942 - accuracy: 0.7500\n","Epoch 33/100\n","1/1 [==============================] - 0s 18ms/step - loss: 0.5855 - accuracy: 1.0000\n","Epoch 34/100\n","1/1 [==============================] - 0s 12ms/step - loss: 0.6983 - accuracy: 0.7500\n","Epoch 35/100\n","1/1 [==============================] - 0s 14ms/step - loss: 0.7670 - accuracy: 0.5000\n","Epoch 36/100\n","1/1 [==============================] - 0s 18ms/step - loss: 0.7180 - accuracy: 0.5000\n","Epoch 37/100\n","1/1 [==============================] - 0s 14ms/step - loss: 0.7327 - accuracy: 0.5000\n","Epoch 38/100\n","1/1 [==============================] - 0s 22ms/step - loss: 0.8478 - accuracy: 0.2500\n","Epoch 39/100\n","1/1 [==============================] - 0s 19ms/step - loss: 0.8526 - accuracy: 0.2500\n","Epoch 40/100\n","1/1 [==============================] - 0s 18ms/step - loss: 0.8645 - accuracy: 0.2500\n","Epoch 41/100\n","1/1 [==============================] - 0s 16ms/step - loss: 0.7110 - accuracy: 0.5000\n","Epoch 42/100\n","1/1 [==============================] - 0s 12ms/step - loss: 0.8829 - accuracy: 0.2500\n","Epoch 43/100\n","1/1 [==============================] - 0s 15ms/step - loss: 0.6385 - accuracy: 1.0000\n","Epoch 44/100\n","1/1 [==============================] - 0s 15ms/step - loss: 0.6775 - accuracy: 0.7500\n","Epoch 45/100\n","1/1 [==============================] - 0s 22ms/step - loss: 0.6128 - accuracy: 1.0000\n","Epoch 46/100\n","1/1 [==============================] - 0s 19ms/step - loss: 0.7756 - accuracy: 0.5000\n","Epoch 47/100\n","1/1 [==============================] - 0s 26ms/step - loss: 0.7035 - accuracy: 0.7500\n","Epoch 48/100\n","1/1 [==============================] - 0s 13ms/step - loss: 0.7152 - accuracy: 0.5000\n","Epoch 49/100\n","1/1 [==============================] - 0s 16ms/step - loss: 0.7035 - accuracy: 0.7500\n","Epoch 50/100\n","1/1 [==============================] - 0s 20ms/step - loss: 0.7067 - accuracy: 0.7500\n","Epoch 51/100\n","1/1 [==============================] - 0s 15ms/step - loss: 0.7028 - accuracy: 0.7500\n","Epoch 52/100\n","1/1 [==============================] - 0s 15ms/step - loss: 0.6902 - accuracy: 0.7500\n","Epoch 53/100\n","1/1 [==============================] - 0s 22ms/step - loss: 0.7958 - accuracy: 0.5000\n","Epoch 54/100\n","1/1 [==============================] - 0s 23ms/step - loss: 0.7054 - accuracy: 0.7500\n","Epoch 55/100\n","1/1 [==============================] - 0s 17ms/step - loss: 0.6876 - accuracy: 0.7500\n","Epoch 56/100\n","1/1 [==============================] - 0s 19ms/step - loss: 0.6265 - accuracy: 1.0000\n","Epoch 57/100\n","1/1 [==============================] - 0s 30ms/step - loss: 0.6627 - accuracy: 1.0000\n","Epoch 58/100\n","1/1 [==============================] - 0s 19ms/step - loss: 0.6685 - accuracy: 0.7500\n","Epoch 59/100\n","1/1 [==============================] - 0s 16ms/step - loss: 0.6887 - accuracy: 0.7500\n","Epoch 60/100\n","1/1 [==============================] - 0s 22ms/step - loss: 0.6761 - accuracy: 0.7500\n","Epoch 61/100\n","1/1 [==============================] - 0s 14ms/step - loss: 0.6403 - accuracy: 1.0000\n","Epoch 62/100\n","1/1 [==============================] - 0s 20ms/step - loss: 0.6488 - accuracy: 0.7500\n","Epoch 63/100\n","1/1 [==============================] - 0s 17ms/step - loss: 0.6835 - accuracy: 0.7500\n","Epoch 64/100\n","1/1 [==============================] - 0s 12ms/step - loss: 0.6935 - accuracy: 0.7500\n","Epoch 65/100\n","1/1 [==============================] - 0s 15ms/step - loss: 0.7251 - accuracy: 0.5000\n","Epoch 66/100\n","1/1 [==============================] - 0s 13ms/step - loss: 0.8066 - accuracy: 0.2500\n","Epoch 67/100\n","1/1 [==============================] - 0s 16ms/step - loss: 0.7154 - accuracy: 0.7500\n","Epoch 68/100\n","1/1 [==============================] - 0s 17ms/step - loss: 0.7069 - accuracy: 0.7500\n","Epoch 69/100\n","1/1 [==============================] - 0s 17ms/step - loss: 0.7298 - accuracy: 0.7500\n","Epoch 70/100\n","1/1 [==============================] - 0s 13ms/step - loss: 0.6263 - accuracy: 0.7500\n","Epoch 71/100\n","1/1 [==============================] - 0s 18ms/step - loss: 0.7187 - accuracy: 0.7500\n","Epoch 72/100\n","1/1 [==============================] - 0s 16ms/step - loss: 0.6484 - accuracy: 0.5000\n","Epoch 73/100\n","1/1 [==============================] - 0s 16ms/step - loss: 0.5788 - accuracy: 1.0000\n","Epoch 74/100\n","1/1 [==============================] - 0s 18ms/step - loss: 0.7254 - accuracy: 0.5000\n","Epoch 75/100\n","1/1 [==============================] - 0s 18ms/step - loss: 0.6594 - accuracy: 0.5000\n","Epoch 76/100\n","1/1 [==============================] - 0s 16ms/step - loss: 0.6785 - accuracy: 0.7500\n","Epoch 77/100\n","1/1 [==============================] - 0s 17ms/step - loss: 0.6186 - accuracy: 1.0000\n","Epoch 78/100\n","1/1 [==============================] - 0s 14ms/step - loss: 0.7620 - accuracy: 0.5000\n","Epoch 79/100\n","1/1 [==============================] - 0s 15ms/step - loss: 0.6299 - accuracy: 1.0000\n","Epoch 80/100\n","1/1 [==============================] - 0s 24ms/step - loss: 0.6664 - accuracy: 1.0000\n","Epoch 81/100\n","1/1 [==============================] - 0s 14ms/step - loss: 0.6102 - accuracy: 1.0000\n","Epoch 82/100\n","1/1 [==============================] - 0s 22ms/step - loss: 0.7155 - accuracy: 0.5000\n","Epoch 83/100\n","1/1 [==============================] - 0s 14ms/step - loss: 0.7063 - accuracy: 0.7500\n","Epoch 84/100\n","1/1 [==============================] - 0s 18ms/step - loss: 0.6616 - accuracy: 0.7500\n","Epoch 85/100\n","1/1 [==============================] - 0s 13ms/step - loss: 0.6877 - accuracy: 0.7500\n","Epoch 86/100\n","1/1 [==============================] - 0s 17ms/step - loss: 0.6510 - accuracy: 1.0000\n","Epoch 87/100\n","1/1 [==============================] - 0s 18ms/step - loss: 0.7072 - accuracy: 0.5000\n","Epoch 88/100\n","1/1 [==============================] - 0s 16ms/step - loss: 0.5948 - accuracy: 1.0000\n","Epoch 89/100\n","1/1 [==============================] - 0s 16ms/step - loss: 0.6545 - accuracy: 1.0000\n","Epoch 90/100\n","1/1 [==============================] - 0s 14ms/step - loss: 0.7391 - accuracy: 0.5000\n","Epoch 91/100\n","1/1 [==============================] - 0s 12ms/step - loss: 0.7455 - accuracy: 0.5000\n","Epoch 92/100\n","1/1 [==============================] - 0s 11ms/step - loss: 0.7139 - accuracy: 0.7500\n","Epoch 93/100\n","1/1 [==============================] - 0s 15ms/step - loss: 0.7170 - accuracy: 0.5000\n","Epoch 94/100\n","1/1 [==============================] - 0s 14ms/step - loss: 0.6888 - accuracy: 0.5000\n","Epoch 95/100\n","1/1 [==============================] - 0s 13ms/step - loss: 0.6741 - accuracy: 0.7500\n","Epoch 96/100\n","1/1 [==============================] - 0s 15ms/step - loss: 0.6649 - accuracy: 0.7500\n","Epoch 97/100\n","1/1 [==============================] - 0s 23ms/step - loss: 0.5843 - accuracy: 0.7500\n","Epoch 98/100\n","1/1 [==============================] - 0s 20ms/step - loss: 0.7671 - accuracy: 0.5000\n","Epoch 99/100\n","1/1 [==============================] - 0s 14ms/step - loss: 0.8111 - accuracy: 0.5000\n","Epoch 100/100\n","1/1 [==============================] - 0s 15ms/step - loss: 0.6742 - accuracy: 0.7500\n","1/1 [==============================] - 0s 265ms/step - loss: 0.6273 - accuracy: 1.0000\n","Loss: 0.6273075342178345, Accuracy: 1.0\n","1/1 [==============================] - 0s 162ms/step\n","Predictions:\n","[[0.4797191 ]\n"," [0.5987721 ]\n"," [0.52818763]\n"," [0.4392751 ]]\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras import regularizers\n","\n","\"\"\"\n","Feedforward Neural Network (FNN) Example with Dropout Regularization using TensorFlow:\n","\n","Input Data:\n","- X_train: Input features (2-dimensional array)\n","- y_train: Target labels (binary classification, 1-dimensional array)\n","\n","Model Architecture:\n","- Input Layer: 2 neurons corresponding to the input features\n","- Hidden Layer 1: Dense layer with 64 neurons and ReLU activation function\n","- Dropout Layer 1: Regularization layer with dropout rate of 0.5\n","- Hidden Layer 2: Dense layer with 32 neurons and ReLU activation function\n","- Dropout Layer 2: Regularization layer with dropout rate of 0.5\n","- Output Layer: Dense layer with 1 neuron and sigmoid activation function (binary classification)\n","\n","Training Parameters:\n","- Loss function: Binary crossentropy\n","- Optimizer: Adam optimizer\n","- Regularization: L2 regularization with regularization rate of 0.001 applied to kernel weights of dense layers\n","- Batch size: 4\n","- Number of epochs: 100\n","\n","Benefits:\n","- Dropout regularization helps prevent overfitting by randomly setting a fraction of input units to 0 during training.\n","- L2 regularization penalizes large weights in the model, promoting simpler models and reducing overfitting.\n","- Adam optimizer is known for its efficiency and effectiveness in training neural networks.\n","- ReLU activation function is commonly used in hidden layers of neural networks and helps alleviate the vanishing gradient problem.\n","- Sigmoid activation function in the output layer is suitable for binary classification tasks.\n","\n","Alternatives:\n","- Regularization: Besides L2 regularization, alternatives like L1 regularization or a combination of both can be used.\n","- Activation functions: Other activation functions like tanh or Leaky ReLU can be experimented with.\n","- Optimizers: Besides Adam optimizer, alternatives like SGD (Stochastic Gradient Descent) or RMSprop can be used.\n","- Model architecture: Experimenting with different numbers of layers and neurons can help find the optimal architecture for the given task.\n","\n","\"\"\"\n","\n","\n","\n","\n","\n","\n","\n","\n","# Create sample data\n","X_train = [[0, 0], [0, 1], [1, 0], [1, 1]]\n","y_train = [[0], [1], [1], [0]]\n","\n","\n","\n","\n","\n","\n","\n","\n","# Build the model\n","model = Sequential([\n","    Dense(64, activation='relu', input_shape=(2,), kernel_regularizer=regularizers.l2(0.001)),\n","    Dropout(0.5),\n","    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n","    Dropout(0.5),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","\"\"\"\n","Activation Functions for Neural Networks:\n","\n","Activation functions introduce non-linearity into neural network models, enabling them to learn complex patterns and relationships in the data. Different activation functions have unique properties that impact the model's training dynamics, convergence behavior, and performance.\n","\n","1. Sigmoid:\n","   - Description: Sigmoid function squashes the input values to the range [0, 1].\n","   - Use Cases: Sigmoid activation is commonly used in the output layer of binary classification tasks.\n","   - Alternatives: Other activation functions like Tanh or ReLU are preferred in hidden layers due to the vanishing gradient problem.\n","   - Pros: Sigmoid activation produces probabilities that are interpretable as class probabilities in binary classification tasks.\n","   - Cons: Sigmoid activation suffers from the vanishing gradient problem, leading to slow convergence and potential saturation of gradients.\n","\n","2. Tanh (Hyperbolic Tangent):\n","   - Description: Tanh function squashes the input values to the range [-1, 1].\n","   - Use Cases: Tanh activation is often used in hidden layers of neural networks.\n","   - Alternatives: Tanh can be replaced by other activation functions like ReLU or Leaky ReLU to mitigate the vanishing gradient problem.\n","   - Pros: Tanh activation introduces stronger non-linearity compared to sigmoid, making it effective for capturing complex patterns in the data.\n","   - Cons: Tanh activation also suffers from the vanishing gradient problem, especially in deep networks with many layers.\n","\n","3. ReLU (Rectified Linear Unit):\n","   - Description: ReLU function returns 0 for negative input values and the input value itself for positive values.\n","   - Use Cases: ReLU activation is widely used in hidden layers of deep neural networks.\n","   - Alternatives: Leaky ReLU, Parametric ReLU (PReLU), and Exponential ReLU (ELU) are variations of ReLU that address some of its limitations.\n","   - Pros: ReLU activation accelerates the convergence of training due to its non-saturating nature and sparsity of activation.\n","   - Cons: ReLU can suffer from the dying ReLU problem, where neurons become inactive and stop updating their weights during training.\n","\n","4. Leaky ReLU:\n","   - Description: Leaky ReLU allows a small, non-zero gradient for negative input values to prevent neurons from becoming completely inactive.\n","   - Use Cases: Leaky ReLU is used as an alternative to ReLU to mitigate the dying ReLU problem.\n","   - Alternatives: Parametric ReLU (PReLU) and Exponential ReLU (ELU) are other alternatives that address similar issues.\n","   - Pros: Leaky ReLU prevents neurons from dying during training, leading to more stable convergence and better performance.\n","   - Cons: Leaky ReLU introduces an additional hyperparameter (leakiness) that needs to be tuned, which can increase model complexity.\n","\n","5. Softmax:\n","   - Description: Softmax function computes the probability distribution over multiple classes, ensuring that the sum of probabilities equals 1.\n","   - Use Cases: Softmax activation is used in the output layer of multi-class classification tasks.\n","   - Alternatives: Sigmoid activation can be used in binary classification tasks, while softmax is preferred for multi-class classification.\n","   - Pros: Softmax activation provides a probability distribution over multiple classes, making it suitable for multi-class classification.\n","   - Cons: Softmax activation can suffer from numerical instability when dealing with large or very small input values, leading to potential overflow or underflow issues.\n","\n","6. Linear:\n","   - Description: Linear activation function returns the input value itself without any transformation.\n","   - Use Cases: Linear activation is commonly used in the output layer for regression tasks.\n","   - Alternatives: Sigmoid or softmax activations are used for classification tasks.\n","   - Pros: Linear activation produces unbounded output values, making it suitable for regression tasks where the target values are continuous.\n","   - Cons: Linear activation may not be suitable for classification tasks where non-linear decision boundaries are required.\n","\n","Kernel Regularization Techniques for Neural Networks:\n","\n","Kernel regularization methods are used in neural networks to prevent overfitting by penalizing large weights or imposing constraints on the network's parameters. These techniques help improve the generalization performance of the model by reducing its complexity and preventing it from memorizing the training data.\n","\n","1. L1 Regularization (Lasso):\n","   - Description: L1 regularization adds the absolute value of the weights as a penalty term to the loss function.\n","   - Use Cases: L1 regularization is effective when the goal is to induce sparsity in the model, i.e., to encourage some weights to be exactly zero.\n","   - Alternatives: L2 regularization, Elastic Net regularization.\n","   - Pros: L1 regularization can lead to sparse solutions, making the model more interpretable and memory-efficient.\n","   - Cons: L1 regularization tends to select only a subset of features, which may discard potentially useful information and reduce model accuracy.\n","\n","2. L2 Regularization (Ridge):\n","   - Description: L2 regularization adds the squared magnitude of the weights as a penalty term to the loss function.\n","   - Use Cases: L2 regularization is widely used to prevent overfitting in neural networks and is effective in improving model generalization.\n","   - Alternatives: L1 regularization, Elastic Net regularization.\n","   - Pros: L2 regularization penalizes large weights more smoothly than L1 regularization, leading to better numerical stability during training.\n","   - Cons: L2 regularization may not induce sparsity as effectively as L1 regularization, and the resulting models may be less interpretable.\n","\n","3. Elastic Net Regularization:\n","   - Description: Elastic Net regularization combines L1 and L2 regularization by adding both the absolute and squared magnitudes of the weights to the loss function.\n","   - Use Cases: Elastic Net regularization is useful when there are many correlated features, as it can select groups of correlated features together.\n","   - Alternatives: L1 regularization, L2 regularization.\n","   - Pros: Elastic Net regularization combines the benefits of L1 and L2 regularization, providing a balance between sparsity and smoothness in the learned weights.\n","   - Cons: Elastic Net regularization introduces an additional hyperparameter (mixing ratio) that needs to be tuned, increasing the complexity of the model.\n","\n","4. Dropout:\n","   - Description: Dropout randomly sets a fraction of input units to zero during training to prevent co-adaptation of neurons and encourage robustness.\n","   - Use Cases: Dropout is commonly used in deep neural networks, especially when dealing with large datasets and complex architectures.\n","   - Alternatives: Weight decay, Batch normalization.\n","   - Pros: Dropout acts as a form of ensemble learning by training multiple subnetworks, leading to improved generalization and reduced overfitting.\n","   - Cons: Dropout increases training time since each forward pass involves randomly dropping units, and it requires careful tuning of the dropout rate.\n","\"\"\"\n","\n","\"\"\"\n","Layers in Feedforward Neural Networks (FNN):\n","\n","1. Dense Layer:\n","   - Description: Dense layer, also known as a fully connected layer, connects each neuron in the current layer to every neuron in the next layer.\n","   - Pros:\n","     - Versatile: Can model complex relationships between input and output.\n","     - Suitable for various tasks: Can be used for classification, regression, and other tasks.\n","     - Regularization: Can help prevent overfitting with dropout, L1/L2 regularization, etc.\n","   - Cons:\n","     - Parameter-heavy: Requires a large number of parameters, leading to longer training times and potential overfitting.\n","     - Lack of interpretability: Due to the dense connectivity, understanding the role of individual neurons can be challenging.\n","   - Use Cases:\n","     - Image Classification: Dense layers are commonly used in the classification of images.\n","     - Natural Language Processing (NLP): Used in tasks such as sentiment analysis, text classification, etc.\n","     - Regression Tasks: Suitable for predicting continuous variables, such as housing prices, stock prices, etc.\n","\n","2. Dropout Layer:\n","   - Description: Dropout layer randomly sets a fraction of input units to zero during training to prevent overfitting.\n","   - Pros:\n","     - Regularization: Helps prevent overfitting by reducing interdependence between neurons.\n","     - Improved Generalization: Forces the network to learn more robust features.\n","     - Easy to Implement: Simply insert Dropout layers into the model architecture.\n","   - Cons:\n","     - Increased Training Time: Dropout increases training time since the network needs more epochs to converge.\n","     - Decreased Network Capacity: Dropout can reduce the effective capacity of the network.\n","   - Use Cases:\n","     - Deep Neural Networks: Used in conjunction with dense layers in deep neural networks to prevent overfitting.\n","     - Large Networks: Effective for large networks with many parameters.\n","     - Classification Tasks: Commonly applied in classification tasks to improve generalization.\n","\n","3. Activation Layers (e.g., ReLU, Sigmoid, Tanh):\n","   - Description: Activation layers introduce non-linearity to the network, allowing it to model complex relationships in the data.\n","   - Pros:\n","     - Introduce Non-Linearity: Enable the network to learn complex mappings between inputs and outputs.\n","     - Computationally Efficient: Many activation functions are computationally efficient to compute.\n","     - Differentiability: Activation functions are usually differentiable, allowing for gradient-based optimization.\n","   - Cons:\n","     - Vanishing/Exploding Gradient Problem: Some activation functions can suffer from vanishing or exploding gradients.\n","     - Saturation: Activation functions may saturate in certain regions, leading to slow learning.\n","   - Use Cases:\n","     - ReLU: Widely used in deep learning architectures due to its simplicity and effectiveness.\n","     - Sigmoid: Used in binary classification tasks where the output needs to be between 0 and 1.\n","     - Tanh: Commonly used in recurrent neural networks (RNNs) and LSTMs.\n","\n","\"\"\"\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","\"\"\"\n","Optimizers for Neural Networks:\n","\n","Optimizers are algorithms used to update the parameters (weights and biases) of neural network models during the training process. They play a crucial role in determining how quickly and effectively the model learns from the training data.\n","\n","1. Adam (Adaptive Moment Estimation):\n","   - Description: Adam is an adaptive learning rate optimization algorithm that computes adaptive learning rates for each parameter. It combines ideas from both RMSprop and Momentum methods.\n","   - Use Cases: Adam is widely used in training deep neural networks for various tasks such as image classification, natural language processing, and reinforcement learning.\n","   - Alternatives: Other popular optimizers include Stochastic Gradient Descent (SGD), RMSprop, and AdaGrad.\n","   - Pros: Adam usually converges faster and is less sensitive to the choice of hyperparameters compared to other optimization algorithms. It also performs well on a wide range of problems with minimal hyperparameter tuning.\n","   - Cons: Adam may require more memory due to the additional computations involved. It can also overshoot the minimum due to its adaptive learning rate behavior.\n","\n","2. Stochastic Gradient Descent (SGD):\n","   - Description: SGD is a classic optimization algorithm used to minimize the loss function by updating the model parameters in the direction of the negative gradient of the loss with respect to the parameters.\n","   - Use Cases: SGD is commonly used in training neural networks, especially in settings where computational resources are limited or memory constraints exist.\n","   - Alternatives: Variants of SGD such as Mini-batch SGD, Momentum SGD, and Nesterov Accelerated Gradient (NAG) can be used to improve convergence and stability.\n","   - Pros: SGD is simple and computationally efficient, making it suitable for large-scale machine learning tasks. It is also less prone to overshooting the minimum compared to adaptive methods.\n","   - Cons: SGD may converge slowly and get stuck in local minima, especially in high-dimensional parameter spaces.\n","\n","3. RMSprop (Root Mean Square Propagation):\n","   - Description: RMSprop is an adaptive learning rate optimization algorithm that divides the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight.\n","   - Use Cases: RMSprop is effective for training deep neural networks, particularly in scenarios where the gradients vary widely across different dimensions of the parameter space.\n","   - Alternatives: Adam, SGD, and AdaGrad are alternative optimization algorithms that can be used in place of RMSprop.\n","   - Pros: RMSprop adapts the learning rate independently for each parameter, making it well-suited for non-stationary and noisy objective functions.\n","   - Cons: RMSprop may require more tuning of hyperparameters compared to Adam. It can also converge prematurely in some cases, leading to suboptimal solutions.\n","\n","4. AdaGrad (Adaptive Gradient Algorithm):\n","   - Description: AdaGrad is an adaptive learning rate optimization algorithm that adapts the learning rates of model parameters based on the historical gradients accumulated for each parameter.\n","   - Use Cases: AdaGrad is suitable for training models with sparse data or when dealing with features that have different frequencies in the input data.\n","   - Alternatives: Adam, RMSprop, and SGD are alternative optimization algorithms that can be used instead of AdaGrad.\n","   - Pros: AdaGrad automatically scales the learning rates of model parameters based on their historical gradients, which can improve convergence on convex optimization problems.\n","   - Cons: AdaGrad may become too conservative in later iterations, resulting in slow convergence. It also accumulates squared gradients over time, potentially leading to vanishing learning rates for frequently occurring features.\n","\n","Loss Functions for Neural Networks:\n","\n","Loss functions, also known as objective functions or cost functions, are used to quantify the difference between the predicted output of a neural network model and the actual target values during the training process. They play a crucial role in guiding the optimization algorithm to update the model parameters to minimize the error.\n","\n","1. Mean Squared Error (MSE):\n","   - Description: MSE computes the average of the squared differences between the predicted and actual target values.\n","   - Use Cases: MSE is commonly used in regression tasks, where the model predicts continuous numeric values.\n","   - Alternatives: Other regression loss functions such as Mean Absolute Error (MAE) and Huber loss can be used as alternatives to MSE.\n","   - Pros: MSE penalizes large errors more heavily, making it sensitive to outliers and suitable for tasks where precise estimation is required.\n","   - Cons: MSE is sensitive to outliers and can be influenced by the scale of the target variable, leading to biased models.\n","\n","2. Binary Cross-Entropy Loss (Binary CE):\n","   - Description: Binary CE computes the cross-entropy loss between the predicted probabilities and the binary (0/1) target labels.\n","   - Use Cases: Binary CE is commonly used in binary classification tasks, where the model predicts the probability of a binary outcome.\n","   - Alternatives: Other binary classification loss functions such as Hinge loss and Squared Hinge loss can be used for linear classifiers.\n","   - Pros: Binary CE encourages the model to produce high probabilities for the correct class, making it effective for training binary classifiers.\n","   - Cons: Binary CE can suffer from vanishing gradients when the predicted probabilities are close to the true labels, leading to slow convergence.\n","\n","3. Categorical Cross-Entropy Loss (Categorical CE):\n","   - Description: Categorical CE computes the cross-entropy loss between the predicted probabilities and the one-hot encoded target labels.\n","   - Use Cases: Categorical CE is commonly used in multi-class classification tasks, where the model predicts the probability distribution over multiple classes.\n","   - Alternatives: Other multi-class classification loss functions such as Sparse Categorical Cross-Entropy and Kullback-Leibler divergence can be used for similar tasks.\n","   - Pros: Categorical CE encourages the model to produce sharp and accurate probability distributions over multiple classes, making it suitable for multi-class classification.\n","   - Cons: Categorical CE requires one-hot encoded target labels, which may increase memory consumption and computational overhead.\n","\n","4. Huber Loss:\n","   - Description: Huber loss is a combination of Mean Squared Error (MSE) and Mean Absolute Error (MAE), where the loss is quadratic for small errors and linear for large errors.\n","   - Use Cases: Huber loss is commonly used in regression tasks, particularly when the dataset contains outliers or noisy data points.\n","   - Alternatives: Other robust regression loss functions such as Quantile loss and Tukey's biweight loss can be used for similar purposes.\n","   - Pros: Huber loss is less sensitive to outliers compared to MSE, making it more robust in the presence of noisy data.\n","   - Cons: Huber loss requires tuning of a hyperparameter (delta) to balance between the quadratic and linear loss components, which can affect model performance.\n","\n","Evaluation Metrics for Neural Networks:\n","\n","Evaluation metrics are used to assess the performance of a neural network model on a specific task, such as classification or regression. These metrics provide valuable insights into the model's predictive accuracy, generalization capability, and overall effectiveness.\n","\n","1. Accuracy:\n","   - Description: Accuracy measures the proportion of correctly classified samples out of the total number of samples.\n","   - Use Cases: Accuracy is commonly used in classification tasks to evaluate the overall correctness of the model's predictions.\n","   - Alternatives: Other classification metrics such as Precision, Recall, F1 Score, and Area Under the ROC Curve (AUC-ROC) can provide additional insights into the model's performance.\n","   - Pros: Accuracy provides a simple and intuitive measure of the model's classification performance, making it easy to interpret.\n","   - Cons: Accuracy may not be suitable for imbalanced datasets, where the class distribution is skewed, as it can be biased towards the majority class.\n","\n","2. Precision:\n","   - Description: Precision measures the proportion of true positive predictions among all positive predictions made by the model.\n","   - Use Cases: Precision is useful in scenarios where the cost of false positive predictions is high, such as medical diagnosis or fraud detection.\n","   - Alternatives: Precision is often used in conjunction with Recall to compute the F1 Score, which provides a balanced measure of both precision and recall.\n","   - Pros: Precision focuses on the accuracy of positive predictions, making it valuable in scenarios where false positives are costly.\n","   - Cons: Precision alone may not provide a complete picture of the model's performance, especially in cases where false negatives are also important.\n","\n","3. Recall (Sensitivity):\n","   - Description: Recall measures the proportion of true positive predictions among all actual positive instances in the dataset.\n","   - Use Cases: Recall is important in scenarios where the cost of false negative predictions is high, such as disease detection or anomaly detection.\n","   - Alternatives: Recall is often used in conjunction with Precision to compute the F1 Score, which provides a balanced measure of both precision and recall.\n","   - Pros: Recall focuses on the ability of the model to correctly identify positive instances, making it valuable in scenarios where false negatives are critical.\n","   - Cons: Recall alone may not provide a complete picture of the model's performance, especially in cases where false positives are also important.\n","\n","4. Mean Squared Error (MSE):\n","   - Description: MSE measures the average of the squared differences between the predicted and actual target values.\n","   - Use Cases: MSE is commonly used in regression tasks to quantify the model's predictive accuracy and goodness of fit.\n","   - Alternatives: Other regression evaluation metrics such as Mean Absolute Error (MAE), R-squared (Coefficient of Determination), and Root Mean Squared Error (RMSE) can provide alternative measures of regression performance.\n","   - Pros: MSE penalizes large prediction errors more heavily, making it sensitive to outliers and useful for assessing the model's performance.\n","   - Cons: MSE is influenced by the scale of the target variable, which can make it difficult to compare across different datasets or models.\n","\"\"\"\n","\n","\n","\n","\n","\n","# Train the model\n","model.fit(X_train, y_train, epochs=100, batch_size=4, verbose=1)\n","\n","\"\"\"\n","Model Fit Parameters:\n","\n","1. x: Input data.\n","   - Description: The input data used for training the model. It could be a NumPy array or a TensorFlow tensor.\n","   - Example: x_train (NumPy array), tf.data.Dataset object containing input data.\n","\n","2. y: Target data (labels).\n","   - Description: The target data (labels) corresponding to the input data. It could be a NumPy array or a TensorFlow tensor.\n","   - Example: y_train (NumPy array), tf.data.Dataset object containing target data.\n","\n","3. batch_size: Number of samples per gradient update.\n","   - Description: Defines the number of samples that will be propagated through the network before a gradient update is performed.\n","   - Example: 32, 64, 128.\n","\n","4. epochs: Number of epochs to train the model.\n","   - Description: An epoch is one complete pass through the entire training dataset.\n","   - Example: 10, 20, 50.\n","\n","5. verbose: Verbosity mode.\n","   - Description: Controls the amount of information printed during training.\n","   - Options: 0 (silent), 1 (progress bar), 2 (one line per epoch).\n","   - Example: 1.\n","\n","6. callbacks: List of callbacks to apply during training.\n","   - Description: Callbacks are objects that perform actions at various stages of training.\n","   - Example: [ModelCheckpoint(), EarlyStopping(), TensorBoard()].\n","\n","7. validation_data: Data on which to evaluate the model at the end of each epoch.\n","   - Description: It can be a tuple (x_val, y_val) or a data generator.\n","   - Example: (x_val, y_val), validation_data_generator.\n","\n","8. validation_split: Fraction of the training data to be used as validation data.\n","   - Description: The model will set apart this fraction of the training data and will evaluate on it at the end of each epoch.\n","   - Example: 0.1 (10% of training data used for validation).\n","\n","9. shuffle: Whether to shuffle the training data before each epoch.\n","   - Description: If True, the training data will be shuffled before each epoch.\n","   - Example: True, False.\n","\n","10. initial_epoch: Epoch at which to start training.\n","    - Description: Useful for resuming a previous training run.\n","    - Example: 0, 5 (start training from the 5th epoch).\n","\n","11. steps_per_epoch: Number of steps (batches) to yield from the generator before declaring one epoch finished.\n","    - Description: It can be used when training with a generator.\n","    - Example: 100, 200.\n","\n","12. validation_steps: Number of steps (batches) to yield from the validation data generator at the end of each epoch.\n","    - Description: It can be used when validating with a generator.\n","    - Example: 50, 100.\n","\n","Callbacks:\n","\n","Callbacks are objects that can perform actions at various stages during training, such as at the start or end of each epoch, before or after a batch is processed, etc. They provide a way to customize the behavior of the training process without modifying the model itself.\n","\n","Options:\n","- ModelCheckpoint: Saves the model after every epoch or only when an improvement is observed on the validation set.\n","- EarlyStopping: Stops training when a monitored metric has stopped improving on the validation data.\n","- TensorBoard: Logs training metrics and visualizes them using TensorBoard.\n","- LearningRateScheduler: Dynamically adjusts the learning rate during training.\n","- ReduceLROnPlateau: Reduces the learning rate when a metric has stopped improving.\n","- CSVLogger: Streams epoch results to a CSV file.\n","- RemoteMonitor: Streams epoch results to a server for remote monitoring.\n","- LambdaCallback: Allows you to define custom callback functions on-the-fly.\n","- Custom Callbacks: You can create your own custom callback by subclassing the keras.callbacks.Callback class.\n","\n","Use Cases:\n","- ModelCheckpoint: Useful for saving the model's weights during training to ensure no progress is lost.\n","- EarlyStopping: Prevents overfitting by stopping training when the model's performance on the validation data starts to degrade.\n","- TensorBoard: Provides visualization tools to monitor training and helps in debugging model performance.\n","- LearningRateScheduler: Allows for dynamic adjustment of learning rate based on training progress.\n","- ReduceLROnPlateau: Helps in fine-tuning the learning rate to accelerate training convergence.\n","- CSVLogger: Keeps track of training history for later analysis.\n","- RemoteMonitor: Useful for monitoring training progress remotely.\n","- LambdaCallback: Provides flexibility to define custom actions at different stages of training.\n","- Custom Callbacks: Allows for implementing complex behaviors during training, such as custom learning rate schedules, data augmentation, etc.\n","\n","Pros:\n","- Enhances flexibility by allowing customized behavior during training.\n","- Helps in implementing advanced training strategies like learning rate scheduling, model checkpointing, etc.\n","- Facilitates monitoring and visualization of training metrics.\n","- Enables early stopping to prevent overfitting and save computational resources.\n","\n","Cons:\n","- May increase complexity in code due to the need to define custom callback functions.\n","- Poorly implemented callbacks may lead to unexpected behavior or errors in training.\n","\n","\"\"\"\n","\n","\n","\n","\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(X_train, y_train)\n","print(f'Loss: {loss}, Accuracy: {accuracy}')\n","\n","\n","# Make predictions\n","predictions = model.predict(X_train)\n","print(\"Predictions:\")\n","print(predictions)"]},{"cell_type":"code","source":["\"\"\"\n","Modified Fully Connected Neural Network (FNN) with Hyperparameter Tuning:\n","\n","1. Hyperparameter Tuning:\n","   - Description: Hyperparameter tuning involves optimizing the model's hyperparameters to improve performance.\n","   - Use Cases:\n","     - Enhancing Model Performance: Improves model accuracy, generalization, and convergence speed.\n","     - Preventing Overfitting: Helps prevent overfitting by optimizing regularization parameters.\n","   - Alternatives:\n","     - Grid Search: Exhaustively searches a predefined hyperparameter space.\n","     - Random Search: Randomly samples hyperparameters from a distribution.\n","     - Bayesian Optimization: Uses probabilistic models to search for optimal hyperparameters.\n","\n","2. Number of Layers:\n","   - Description: Specifies the number of hidden layers in the neural network architecture.\n","   - Use Cases:\n","     - Model Complexity: Determines the complexity and capacity of the model.\n","     - Feature Representation: Controls the level of abstraction and representation power of the network.\n","   - Alternatives:\n","     - Fewer Layers: Simpler models with fewer layers may generalize better on small datasets.\n","     - More Layers: Deeper networks with more layers can learn complex hierarchical representations.\n","\n","3. Width of Layers:\n","   - Description: Defines the number of neurons (units) in each hidden layer of the neural network.\n","   - Use Cases:\n","     - Model Capacity: Influences the model's capacity to capture complex patterns in the data.\n","     - Computational Efficiency: Larger width may increase computational requirements during training.\n","   - Alternatives:\n","     - Narrow Layers: Smaller width may reduce overfitting but may also limit the model's expressive power.\n","     - Wide Layers: Larger width enables the model to learn intricate feature representations but may lead to overfitting.\n","\n","4. Activation Functions:\n","   - Description: Activation functions introduce non-linearity to the network, allowing it to learn complex mappings between inputs and outputs.\n","   - Use Cases:\n","     - Non-linearity: Facilitates the modeling of non-linear relationships in the data.\n","     - Gradient Propagation: Ensures stable gradient flow during backpropagation.\n","   - Alternatives:\n","     - ReLU (Rectified Linear Unit): Commonly used due to its simplicity and effectiveness in mitigating vanishing gradients.\n","     - Sigmoid: Suitable for binary classification tasks, squashes output values to the range [0, 1].\n","     - Tanh (Hyperbolic Tangent): Similar to sigmoid but squashes output values to the range [-1, 1].\n","\n","5. Dropout Layers:\n","   - Description: Dropout layers randomly deactivate a fraction of neurons during training, reducing overfitting by promoting model generalization.\n","   - Use Cases:\n","     - Regularization: Prevents co-adaptation of neurons and encourages robustness in the network.\n","     - Ensemble Learning: Mimics the effect of training multiple networks with different subsets of neurons.\n","   - Alternatives:\n","     - L2 Regularization: Penalizes large weights in the network, discouraging overfitting by reducing model complexity.\n","     - Batch Normalization: Normalizes activations between layers, improving stability and accelerating convergence.\n","     - Early Stopping: Halts training when validation performance stops improving, preventing overfitting.\n","\"\"\"\n","\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras import regularizers\n","from sklearn.model_selection import GridSearchCV\n","from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n","from tensorflow.keras.optimizers import Adam\n","\n","def create_model(layers=2, layer_width=64, activation='relu', dropout_rate=0.5, learning_rate=0.001):\n","    \"\"\"\n","    Function to create a Keras Sequential model with configurable architecture.\n","\n","    Parameters:\n","    - layers: Number of hidden layers in the model (default: 2)\n","    - layer_width: Width of each hidden layer (default: 64)\n","    - activation: Activation function for hidden layers (default: 'relu')\n","    - dropout_rate: Dropout rate for dropout layers (default: 0.5)\n","    - learning_rate: Learning rate for the optimizer (default: 0.001)\n","\n","    Returns:\n","    - Keras Sequential model\n","    \"\"\"\n","    model = Sequential()\n","    # Input layer\n","    model.add(Dense(layer_width, activation=activation, input_shape=(2,), kernel_regularizer=regularizers.l2(0.001)))\n","    model.add(Dropout(dropout_rate))\n","    # Hidden layers\n","    for _ in range(layers - 1):\n","        model.add(Dense(layer_width, activation=activation, kernel_regularizer=regularizers.l2(0.001)))\n","        model.add(Dropout(dropout_rate))\n","    # Output layer\n","    model.add(Dense(1, activation='sigmoid'))\n","    # Compile the model\n","    optimizer = Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","# Create sample data\n","X_train = [[0, 0], [0, 1], [1, 0], [1, 1]]\n","y_train = [[0], [1], [1], [0]]\n","\n","# Define hyperparameters for tuning\n","param_grid = {\n","    'layers': [1, 2, 3],\n","    'layer_width': [32, 64, 128],\n","    'activation': ['relu', 'tanh'],\n","    'dropout_rate': [0.2, 0.5, 0.8],\n","    'learning_rate': [0.001, 0.01, 0.1],\n","    'batch_size': [4, 8, 16]\n","}\n","\n","# Create KerasClassifier\n","keras_clf = KerasClassifier(build_fn=create_model, epochs=100, verbose=0)\n","\n","# Perform grid search cross-validation\n","grid = GridSearchCV(estimator=keras_clf, param_grid=param_grid, cv=3)\n","grid_result = grid.fit(X_train, y_train)\n","\n","# Print best results\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","means = grid_result.cv_results_['mean_test_score']\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"],"metadata":{"id":"vxRXvvthTGgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras import regularizers\n","from sklearn.model_selection import GridSearchCV\n","from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n","from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n","\n","def create_model(optimizer='adam', layers=2, layer_width=64, activation='relu', dropout_rate=0.5, learning_rate=0.001):\n","    \"\"\"\n","    Function to create a Keras Sequential model with configurable architecture.\n","\n","    Parameters:\n","    - optimizer: Optimizer type (default: 'adam')\n","    - layers: Number of hidden layers in the model (default: 2)\n","    - layer_width: Width of each hidden layer (default: 64)\n","    - activation: Activation function for hidden layers (default: 'relu')\n","    - dropout_rate: Dropout rate for dropout layers (default: 0.5)\n","    - learning_rate: Learning rate for the optimizer (default: 0.001)\n","\n","    Returns:\n","    - Keras Sequential model\n","    \"\"\"\n","    model = Sequential()\n","    # Input layer\n","    model.add(Dense(layer_width, activation=activation, input_shape=(2,), kernel_regularizer=regularizers.l2(0.001)))\n","    model.add(Dropout(dropout_rate))\n","    # Hidden layers\n","    for _ in range(layers - 1):\n","        model.add(Dense(layer_width, activation=activation, kernel_regularizer=regularizers.l2(0.001)))\n","        model.add(Dropout(dropout_rate))\n","    # Output layer\n","    model.add(Dense(1, activation='sigmoid'))\n","    # Compile the model\n","    model.compile(optimizer=optimizer(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","# Create sample data\n","X_train = [[0, 0], [0, 1], [1, 0], [1, 1]]\n","y_train = [[0], [1], [1], [0]]\n","\n","# Define hyperparameters for tuning\n","param_grid = {\n","    'optimizer': [Adam, RMSprop, SGD],\n","    'layers': [1, 2, 3],\n","    'layer_width': [32, 64, 128],\n","    'activation': ['relu', 'tanh'],\n","    'dropout_rate': [0.2, 0.5, 0.8],\n","    'learning_rate': [0.001, 0.01, 0.1],\n","    'batch_size': [8, 16, 32],  # Add batch size options\n","    'epochs': [100],  # Number of epochs for training\n","    'validation_split': [0.2]  # Validation split ratio\n","}\n","\n","# Create KerasClassifier\n","keras_clf = KerasClassifier(build_fn=create_model, verbose=0)\n","\n","# Perform grid search cross-validation\n","grid = GridSearchCV(estimator=keras_clf, param_grid=param_grid, cv=3)\n","grid_result = grid.fit(X_train, y_train)\n","\n","# Print best results\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","means = grid_result.cv_results_['mean_test_score']\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"],"metadata":{"id":"nw9pf2tlVJsh"},"execution_count":null,"outputs":[]}]}