{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNLbNPv/UaBKPjzI6Hmr70K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"cUnutuWRg3Ua","executionInfo":{"status":"ok","timestamp":1712470026566,"user_tz":0,"elapsed":328,"user":{"displayName":"Márk Németh","userId":"08804061133513418249"}},"outputId":"d0878bb6-0450-4d40-e477-9c3bc6873edb"},"outputs":[{"output_type":"stream","name":"stdout","text":["   ID   Name   Age Gender  Income    Education  Score Label Has_Car Location  \\\n","0   1   John  25.0      M   50000     Bachelor     85   Yes    True     City   \n","1   2  Alice  30.0      F   60000       Master     90    No     NaN   Suburb   \n","2   3    Bob   NaN      M  200000  High School     75   Yes    True    Rural   \n","3   4   Mary  40.0      F   70000          PhD     50   Yes    True     City   \n","4   5   Jane  35.0      F   80000     Bachelor     80    No    True   Suburb   \n","5   1   John  25.0      M   50000     Bachelor     85   Yes    True     City   \n","6   2  Alice  30.0      F   60000       Master     90    No   False   Suburb   \n","\n","   High_Income  \n","0        False  \n","1        False  \n","2         True  \n","3         True  \n","4         True  \n","5        False  \n","6        False  \n","One-Hot Encoded DataFrame:\n","   ID   Name   Age  Income  Score Label Has_Car  High_Income  Gender_F  \\\n","0   1   John  25.0   50000     85   Yes    True        False     False   \n","1   2  Alice  30.0   60000     90    No     NaN        False      True   \n","2   3    Bob   NaN  200000     75   Yes    True         True     False   \n","3   4   Mary  40.0   70000     50   Yes    True         True      True   \n","4   5   Jane  35.0   80000     80    No    True         True      True   \n","5   1   John  25.0   50000     85   Yes    True        False     False   \n","6   2  Alice  30.0   60000     90    No   False        False      True   \n","\n","   Gender_M  Education_Bachelor  Education_High School  Education_Master  \\\n","0      True                True                  False             False   \n","1     False               False                  False              True   \n","2      True               False                   True             False   \n","3     False               False                  False             False   \n","4     False                True                  False             False   \n","5      True                True                  False             False   \n","6     False               False                  False              True   \n","\n","   Education_PhD  Location_City  Location_Rural  Location_Suburb  \n","0          False           True           False            False  \n","1          False          False           False             True  \n","2          False          False            True            False  \n","3           True           True           False            False  \n","4          False          False           False             True  \n","5          False           True           False            False  \n","6          False          False           False             True  \n","\n","Label Encoded DataFrame:\n","   ID   Name   Age  Gender  Income  Education  Score Label Has_Car  Location  \\\n","0   1   John  25.0       1   50000          0     85   Yes    True         0   \n","1   2  Alice  30.0       0   60000          2     90    No     NaN         2   \n","2   3    Bob   NaN       1  200000          1     75   Yes    True         1   \n","3   4   Mary  40.0       0   70000          3     50   Yes    True         0   \n","4   5   Jane  35.0       0   80000          0     80    No    True         2   \n","5   1   John  25.0       1   50000          0     85   Yes    True         0   \n","6   2  Alice  30.0       0   60000          2     90    No   False         2   \n","\n","   High_Income  \n","0        False  \n","1        False  \n","2         True  \n","3         True  \n","4         True  \n","5        False  \n","6        False  \n"]},{"output_type":"execute_result","data":{"text/plain":["'\\nOne-hot encoding: Convert categorical variables into binary vectors.\\n\\nWhen to Use:\\n------------\\n- Use one-hot encoding when dealing with categorical variables that have no ordinal relationship and the algorithm might interpret them as such.\\n- Use it when you want to ensure that categorical variables are treated as distinct entities without any assumed order or hierarchy.\\n- Use it for machine learning algorithms that require numeric input features and may incorrectly interpret categorical variables with numeric labels as having ordinal relationships.\\n- Use it when you want to prevent biases introduced by assumed ordinal relationships and ensure accurate model training and prediction.\\n\\nWhen to Avoid:\\n--------------\\n- Avoid one-hot encoding for categorical variables with a high number of unique categories (high cardinality), as it can lead to a significant increase in dataset dimensionality and computational inefficiencies.\\n- Avoid it when working with tree-based models like random forests on datasets with extremely high cardinality categorical variables, as it may become computationally expensive or prone to overfitting.\\n- Avoid one-hot encoding for linear models, especially when applying regularization techniques like Lasso, as it can lead to multicollinearity and less interpretable models.\\n- Avoid it if memory or storage constraints are a concern, as one-hot encoding can significantly increase memory and storage requirements, especially for large datasets.\\n- Avoid it when interpretability is crucial, as one-hot encoding can make the resulting model less interpretable due to the binary nature of the encoded features.\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}],"source":["import pandas as pd\n","import numpy as np\n","\n","# Creating sample data\n","data = {\n","    'ID': [1, 2, 3, 4, 5, 1, 2],\n","    'Name': ['John', 'Alice', 'Bob', 'Mary', 'Jane', 'John', 'Alice'],\n","    'Age': [25, 30, np.nan, 40, 35, 25, 30],\n","    'Gender': ['M', 'F', 'M', 'F', 'F', 'M', 'F'],\n","    'Income': [50000, 60000, 45000, 70000, 80000, 50000, 60000],\n","    'Education': ['Bachelor', 'Master', 'High School', 'PhD', 'Bachelor', 'Bachelor', 'Master'],\n","    'Score': [85, 90, 75, 95, 80, 85, 90],\n","    'Label': ['Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No'],\n","    'Has_Car': [True, False, True, True, True, True, False],\n","    'Location': ['City', 'Suburb', 'Rural', 'City', 'Suburb', 'City', 'Suburb'],\n","    'High_Income': [False, False, True, True, True, False, False]  # Target feature\n","}\n","\n","# Adding missing values and outliers\n","data['Age'][2] = np.nan\n","data['Income'][2] = 200000\n","data['Score'][3] = 50\n","data['Has_Car'][1] = np.nan\n","\n","# Creating DataFrame\n","df = pd.DataFrame(data)\n","\n","print(df)\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","# One-hot encoding\n","df_one_hot = pd.get_dummies(df, columns=['Gender', 'Education', 'Location'])\n","# One-hot encoding creates binary columns for each category in the specified columns.\n","# It's useful when the categories have no ordinal relationship and the algorithm might interpret them as such.\n","print(\"One-Hot Encoded DataFrame:\")\n","print(df_one_hot)\n","\n","# Label encoding\n","label_encoder = LabelEncoder()\n","df_label_encoded = df.copy()\n","# Applying label encoding to 'Gender' column\n","df_label_encoded['Gender'] = label_encoder.fit_transform(df_label_encoded['Gender'])\n","# Applying label encoding to 'Education' column\n","df_label_encoded['Education'] = label_encoder.fit_transform(df_label_encoded['Education'])\n","# Applying label encoding to 'Location' column\n","df_label_encoded['Location'] = label_encoder.fit_transform(df_label_encoded['Location'])\n","# Label encoding assigns a unique integer to each category, making it easier for algorithms to process.\n","print(\"\\nLabel Encoded DataFrame:\")\n","print(df_label_encoded)\n","\n","\n","\"\"\"\n","One-hot encoding: Convert categorical variables into binary vectors.\n","\n","When to Use:\n","------------\n","- Use one-hot encoding when dealing with categorical variables that have no ordinal relationship and the algorithm might interpret them as such.\n","- Use it when you want to ensure that categorical variables are treated as distinct entities without any assumed order or hierarchy.\n","- Use it for machine learning algorithms that require numeric input features and may incorrectly interpret categorical variables with numeric labels as having ordinal relationships.\n","- Use it when you want to prevent biases introduced by assumed ordinal relationships and ensure accurate model training and prediction.\n","\n","When to Avoid:\n","--------------\n","- Avoid one-hot encoding for categorical variables with a high number of unique categories (high cardinality), as it can lead to a significant increase in dataset dimensionality and computational inefficiencies.\n","- Avoid it when working with tree-based models like random forests on datasets with extremely high cardinality categorical variables, as it may become computationally expensive or prone to overfitting.\n","- Avoid one-hot encoding for linear models, especially when applying regularization techniques like Lasso, as it can lead to multicollinearity and less interpretable models.\n","- Avoid it if memory or storage constraints are a concern, as one-hot encoding can significantly increase memory and storage requirements, especially for large datasets.\n","- Avoid it when interpretability is crucial, as one-hot encoding can make the resulting model less interpretable due to the binary nature of the encoded features.\n","\n","\"\"\""]},{"cell_type":"code","source":[],"metadata":{"id":"RQqUKKMlLZFd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from scipy.stats import zscore\n","\n","# Method 1: Z-Score\n","# Z-score method identifies outliers based on their deviation from the mean in terms of standard deviation.\n","# Outliers are typically defined as observations with a z-score greater than a certain threshold (e.g., 3).\n","threshold = 3\n","z_scores = zscore(df_label_encoded['Income'])\n","outlier_indices_zscore = np.where(np.abs(z_scores) > threshold)[0]\n","df_without_outliers_zscore = df_label_encoded.drop(outlier_indices_zscore)\n","print(\"\\nDataFrame without outliers using Z-Score method:\")\n","print(df_without_outliers_zscore)\n","\n","# Method 2: Interquartile Range (IQR)\n","# IQR method identifies outliers based on the difference between the third quartile (Q3) and the first quartile (Q1).\n","# Outliers are typically defined as observations that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR.\n","Q1 = df_label_encoded['Income'].quantile(0.25)\n","Q3 = df_label_encoded['Income'].quantile(0.75)\n","IQR = Q3 - Q1\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","outlier_indices_iqr = np.where((df_label_encoded['Income'] < lower_bound) | (df_label_encoded['Income'] > upper_bound))[0]\n","df_without_outliers_iqr = df_label_encoded.drop(outlier_indices_iqr)\n","print(\"\\nDataFrame without outliers using IQR method:\")\n","print(df_without_outliers_iqr)\n","\n","\"\"\"\n","Explanation and Comments:\n","\n","- Z-Score Method:\n","  - Benefits:\n","    - Easily identifies outliers based on their deviation from the mean in terms of standard deviation.\n","    - Provides a standardized measure of how far an observation deviates from the mean, regardless of the original scale of the data.\n","  - Use Cases:\n","    - Useful when the distribution of the data is approximately normal.\n","    - Suitable for datasets where the mean and standard deviation are meaningful measures of central tendency and dispersion.\n","\n","- Interquartile Range (IQR) Method:\n","  - Benefits:\n","    - Robust to outliers and non-normal distributions.\n","    - Provides a measure of the spread of the middle 50% of the data, making it less sensitive to extreme values compared to the range.\n","  - Use Cases:\n","    - Effective for skewed distributions or datasets with a large number of outliers.\n","    - Commonly used in exploratory data analysis and data cleaning processes.\n","\n","Both methods are effective in identifying and handling outliers, but the choice between them depends on the characteristics of the data and the assumptions of the analysis.\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":730},"id":"fj2L4Y20hgeo","executionInfo":{"status":"ok","timestamp":1712470051053,"user_tz":0,"elapsed":328,"user":{"displayName":"Márk Németh","userId":"08804061133513418249"}},"outputId":"1bdbaa1d-dbf2-407f-f715-c14ab8f95325"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","DataFrame without outliers using Z-Score method:\n","   ID   Name   Age  Gender  Income  Education  Score Label Has_Car  Location  \\\n","0   1   John  25.0       1   50000          0     85   Yes    True         0   \n","1   2  Alice  30.0       0   60000          2     90    No     NaN         2   \n","2   3    Bob   NaN       1  200000          1     75   Yes    True         1   \n","3   4   Mary  40.0       0   70000          3     50   Yes    True         0   \n","4   5   Jane  35.0       0   80000          0     80    No    True         2   \n","5   1   John  25.0       1   50000          0     85   Yes    True         0   \n","6   2  Alice  30.0       0   60000          2     90    No   False         2   \n","\n","   High_Income  \n","0        False  \n","1        False  \n","2         True  \n","3         True  \n","4         True  \n","5        False  \n","6        False  \n","\n","DataFrame without outliers using IQR method:\n","   ID   Name   Age  Gender  Income  Education  Score Label Has_Car  Location  \\\n","0   1   John  25.0       1   50000          0     85   Yes    True         0   \n","1   2  Alice  30.0       0   60000          2     90    No     NaN         2   \n","3   4   Mary  40.0       0   70000          3     50   Yes    True         0   \n","4   5   Jane  35.0       0   80000          0     80    No    True         2   \n","5   1   John  25.0       1   50000          0     85   Yes    True         0   \n","6   2  Alice  30.0       0   60000          2     90    No   False         2   \n","\n","   High_Income  \n","0        False  \n","1        False  \n","3         True  \n","4         True  \n","5        False  \n","6        False  \n"]},{"output_type":"execute_result","data":{"text/plain":["'\\nExplanation and Comments:\\n\\n- Z-Score Method:\\n  - Benefits:\\n    - Easily identifies outliers based on their deviation from the mean in terms of standard deviation.\\n    - Provides a standardized measure of how far an observation deviates from the mean, regardless of the original scale of the data.\\n  - Use Cases:\\n    - Useful when the distribution of the data is approximately normal.\\n    - Suitable for datasets where the mean and standard deviation are meaningful measures of central tendency and dispersion.\\n\\n- Interquartile Range (IQR) Method:\\n  - Benefits:\\n    - Robust to outliers and non-normal distributions.\\n    - Provides a measure of the spread of the middle 50% of the data, making it less sensitive to extreme values compared to the range.\\n  - Use Cases:\\n    - Effective for skewed distributions or datasets with a large number of outliers.\\n    - Commonly used in exploratory data analysis and data cleaning processes.\\n\\nBoth methods are effective in identifying and handling outliers, but the choice between them depends on the characteristics of the data and the assumptions of the analysis.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n","\n","# Min-Max scaling\n","scaler = MinMaxScaler()\n","numerical_columns = ['Age', 'Income', 'Score']  # Numerical columns to scale\n","\n","# Apply Min-Max scaling to selected numerical columns in the z-scored DataFrame\n","df_withoutiqroutliers_scaled = df_without_outliers_iqr.copy()\n","\n","# Apply Min-Max scaling to selected numerical columns\n","df_withoutiqroutliers_scaled[numerical_columns] = scaler.fit_transform(df_without_outliers_iqr[numerical_columns])\n","\n","# Print the scaled DataFrame\n","print(\"\\nScaled DataFrame:\")\n","print(df_withoutiqroutliers_scaled)\n","\n","\n","\"\"\"\n","Explanation:\n","\n","- Min-Max Scaling: Min-Max scaling is a technique used to scale numerical features to a specific range, typically between 0 and 1.\n","  It linearly transforms the values so that the minimum value becomes 0 and the maximum value becomes 1, while other values are scaled accordingly.\n","\n","- Benefits:\n","  - Normalization: Min-Max scaling normalizes the features, ensuring that they are within the same scale, which can improve the performance\n","    of certain machine learning algorithms, such as gradient descent-based algorithms.\n","  - Preservation of Relationship: Min-Max scaling preserves the relationship between the original values, as it scales them linearly within\n","    a fixed range. This ensures that the relative differences between values are maintained.\n","\n","- Use Cases:\n","  - Neural Networks: Min-Max scaling is commonly used in neural networks where input features need to be within a consistent range to ensure\n","    stable training.\n","  - Distance-Based Algorithms: It's beneficial for distance-based algorithms such as k-nearest neighbors (KNN) and support vector machines\n","    (SVM), as it ensures that features with larger scales do not dominate the distance calculations.\n","  - Visualization: Min-Max scaling can be useful when visualizing data, as it brings all features within the same scale, making it easier\n","    to compare and interpret the data.\n","\n","- Considerations:\n","  - Sensitivity to Outliers: Min-Max scaling is sensitive to outliers, as it scales the data based on the minimum and maximum values. Therefore,\n","    it's important to handle outliers appropriately before applying Min-Max scaling.\n","  - Suitability for Non-Normal Distributions: It may not be suitable for features with non-normal distributions or features with a large number\n","    of outliers, as it can distort the data distribution. In such cases, other scaling methods like StandardScaler may be more appropriate.\n","\"\"\"\n","\n","\n","\"\"\"\n","Outlier Handling:\n","\n","When to Use:\n","------------\n","- Use outlier handling techniques when dealing with datasets containing observations that deviate significantly from the rest of the data.\n","- Use it to improve the accuracy and reliability of statistical analyses and machine learning models by reducing the impact of outliers on model performance.\n","- Use outlier handling techniques to ensure that statistical assumptions are met and to produce more robust and reliable results in data analysis tasks.\n","\n","When to Avoid:\n","--------------\n","- Avoid outlier handling when outliers are valid and meaningful data points that represent rare but genuine occurrences in the data.\n","- Avoid it when removing outliers would result in the loss of important information or trends in the dataset, leading to biased results or inaccurate conclusions.\n","- Avoid outlier handling if the dataset is small and removing outliers would significantly reduce the sample size, potentially affecting the statistical power of subsequent analyses.\n","- Avoid outlier handling techniques that are overly aggressive or subjective, as they may introduce bias or distort the underlying distribution of the data.\n","\n","Common Outlier Handling Techniques:\n","------------------------------------\n","1. Z-Score Method:\n","   - Benefits: Easily identifies outliers based on their deviation from the mean in terms of standard deviation. Provides a standardized measure of how far an observation deviates from the mean.\n","   - Use Cases: Suitable for datasets with approximately normal distributions and when the mean and standard deviation are meaningful measures of central tendency and dispersion.\n","\n","2. Interquartile Range (IQR) Method:\n","   - Benefits: Robust to outliers and non-normal distributions. Provides a measure of the spread of the middle 50% of the data, making it less sensitive to extreme values.\n","   - Use Cases: Effective for skewed distributions or datasets with a large number of outliers. Commonly used in exploratory data analysis and data cleaning processes.\n","\n","3. Tukey's Fences:\n","   - Benefits: Provides a simple and intuitive method for identifying outliers based on the interquartile range.\n","   - Use Cases: Suitable for identifying outliers in datasets with a relatively large number of observations and when a clear threshold for outlier detection is needed.\n","\n","4. Robust Z-Score:\n","   - Benefits: Similar to the Z-Score method but more robust to outliers and non-normal distributions.\n","   - Use Cases: Useful when the dataset contains extreme outliers that may skew the distribution and when a more robust measure of deviation from the mean is needed.\n","\n","5. Isolation Forest:\n","   - Benefits: Anomaly detection technique that isolates outliers by randomly partitioning the data into subsets. Efficient for high-dimensional datasets and capable of handling mixed data types.\n","   - Use Cases: Suitable for detecting outliers in large datasets with complex and high-dimensional features, such as cybersecurity or fraud detection applications.\n","\n","Considerations:\n","---------------\n","- Consider the characteristics of the dataset, such as distributional properties, sample size, and the presence of influential outliers, when selecting an outlier handling technique.\n","- Evaluate the impact of outlier handling on the validity and reliability of subsequent analyses or model performance.\n","- Document the outlier handling process and rationale to ensure transparency and reproducibility in data analysis workflows.\n","\"\"\"\n","\n","\"\"\"\n","Normalization:\n","\n","When to Use:\n","------------\n","- Use normalization to scale numerical features to a common range, typically between 0 and 1, to ensure consistency in feature scales.\n","- Use it when working with machine learning algorithms that are sensitive to feature scales, such as gradient descent-based algorithms.\n","- Use normalization to improve the convergence speed and stability of optimization algorithms by preventing large-scale features from dominating the optimization process.\n","- Use it to enhance the performance of distance-based algorithms like KNN and SVM, where feature scales can affect the calculation of distances.\n","\n","When to Avoid:\n","--------------\n","- Avoid normalization when the distribution of features is already within a similar scale or when the algorithm is not sensitive to feature scales.\n","- Avoid it if the normalization process would result in the loss of important information or if the original scale of features is meaningful for interpretation.\n","- Avoid normalization if the dataset contains categorical variables or ordinal features that should not be scaled, as it may distort the relationships between variables.\n","\n","Common Normalization Techniques:\n","--------------------------------\n","1. Min-Max Scaling:\n","   - Benefits: Linearly transforms features to a specific range, typically between 0 and 1, preserving the relationship between values.\n","   - Use Cases: Suitable for algorithms that require features to be within a consistent range, such as neural networks and distance-based algorithms.\n","\n","2. Standardization (Z-Score Normalization):\n","   - Benefits: Centers features around the mean and scales them to have a standard deviation of 1, making them more interpretable and robust to outliers.\n","   - Use Cases: Effective for algorithms that assume features are normally distributed or when the scale of features is meaningful for interpretation.\n","\n","3. Robust Scaling:\n","   - Benefits: Scales features based on percentiles, making it robust to outliers and non-normal distributions.\n","   - Use Cases: Suitable for datasets with outliers or skewed distributions, where standardization may be affected by extreme values.\n","\n","Considerations:\n","---------------\n","- Consider the characteristics of the dataset, including the distributional properties of features and the requirements of the algorithm, when selecting a normalization technique.\n","- Evaluate the impact of normalization on the performance and interpretability of machine learning models, and consider alternative scaling methods if necessary.\n","- Document the normalization process and any assumptions made to ensure transparency and reproducibility in data analysis workflows.\n","\"\"\"\n","\n","\n","\"\"\"\n","Scaling:\n","\n","When to Use:\n","------------\n","- Use scaling to transform numerical features into a consistent range to improve the performance and convergence of machine learning algorithms.\n","- Use it when working with algorithms that are sensitive to the scale of features, such as gradient descent-based algorithms and distance-based algorithms.\n","- Use scaling to ensure that all features contribute equally to the model fitting process and to prevent features with larger scales from dominating the optimization process.\n","- Use it to enhance the interpretability and stability of machine learning models by making feature coefficients or importance scores comparable across different features.\n","\n","When to Avoid:\n","--------------\n","- Avoid scaling when the scale of features is not relevant for the problem at hand or when the algorithm is not sensitive to feature scales.\n","- Avoid it if the scaling process would distort the relationships between features or if the original scale of features is meaningful for interpretation.\n","- Avoid scaling categorical variables or ordinal features that should not be transformed, as it may introduce bias or alter the interpretation of these variables.\n","\n","Common Scaling Techniques:\n","---------------------------\n","1. Min-Max Scaling:\n","   - Benefits: Linearly transforms features to a specific range, typically between 0 and 1, preserving the relationship between values.\n","   - Use Cases: Suitable for algorithms that require features to be within a consistent range, such as neural networks and distance-based algorithms.\n","\n","2. Standardization (Z-Score Scaling):\n","   - Benefits: Centers features around the mean and scales them to have a standard deviation of 1, making them more interpretable and robust to outliers.\n","   - Use Cases: Effective for algorithms that assume features are normally distributed or when the scale of features is meaningful for interpretation.\n","\n","3. Robust Scaling:\n","   - Benefits: Scales features based on percentiles, making it robust to outliers and non-normal distributions.\n","   - Use Cases: Suitable for datasets with outliers or skewed distributions, where standardization may be affected by extreme values.\n","\n","Considerations:\n","---------------\n","- Consider the characteristics of the dataset, including the distributional properties of features and the requirements of the algorithm, when selecting a scaling technique.\n","- Evaluate the impact of scaling on the performance and interpretability of machine learning models, and consider alternative scaling methods if necessary.\n","- Document the scaling process and any assumptions made to ensure transparency and reproducibility in data analysis workflows.\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"id":"ou0JJRJByOkw","executionInfo":{"status":"ok","timestamp":1712470410360,"user_tz":0,"elapsed":328,"user":{"displayName":"Márk Németh","userId":"08804061133513418249"}},"outputId":"2173b30a-2a5c-4741-fd68-35ba8eae3397"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Scaled DataFrame:\n","   ID   Name       Age  Gender    Income  Education  Score Label Has_Car  \\\n","0   1   John  0.000000       1  0.000000          0  0.875   Yes    True   \n","1   2  Alice  0.333333       0  0.333333          2  1.000    No     NaN   \n","3   4   Mary  1.000000       0  0.666667          3  0.000   Yes    True   \n","4   5   Jane  0.666667       0  1.000000          0  0.750    No    True   \n","5   1   John  0.000000       1  0.000000          0  0.875   Yes    True   \n","6   2  Alice  0.333333       0  0.333333          2  1.000    No   False   \n","\n","   Location  High_Income  \n","0         0        False  \n","1         2        False  \n","3         0         True  \n","4         2         True  \n","5         0        False  \n","6         2        False  \n"]},{"output_type":"execute_result","data":{"text/plain":["'\\nScaling:\\n\\nWhen to Use:\\n------------\\n- Use scaling to transform numerical features into a consistent range to improve the performance and convergence of machine learning algorithms.\\n- Use it when working with algorithms that are sensitive to the scale of features, such as gradient descent-based algorithms and distance-based algorithms.\\n- Use scaling to ensure that all features contribute equally to the model fitting process and to prevent features with larger scales from dominating the optimization process.\\n- Use it to enhance the interpretability and stability of machine learning models by making feature coefficients or importance scores comparable across different features.\\n\\nWhen to Avoid:\\n--------------\\n- Avoid scaling when the scale of features is not relevant for the problem at hand or when the algorithm is not sensitive to feature scales.\\n- Avoid it if the scaling process would distort the relationships between features or if the original scale of features is meaningful for interpretation.\\n- Avoid scaling categorical variables or ordinal features that should not be transformed, as it may introduce bias or alter the interpretation of these variables.\\n\\nCommon Scaling Techniques:\\n---------------------------\\n1. Min-Max Scaling:\\n   - Benefits: Linearly transforms features to a specific range, typically between 0 and 1, preserving the relationship between values.\\n   - Use Cases: Suitable for algorithms that require features to be within a consistent range, such as neural networks and distance-based algorithms.\\n\\n2. Standardization (Z-Score Scaling):\\n   - Benefits: Centers features around the mean and scales them to have a standard deviation of 1, making them more interpretable and robust to outliers.\\n   - Use Cases: Effective for algorithms that assume features are normally distributed or when the scale of features is meaningful for interpretation.\\n\\n3. Robust Scaling:\\n   - Benefits: Scales features based on percentiles, making it robust to outliers and non-normal distributions.\\n   - Use Cases: Suitable for datasets with outliers or skewed distributions, where standardization may be affected by extreme values.\\n\\nConsiderations:\\n---------------\\n- Consider the characteristics of the dataset, including the distributional properties of features and the requirements of the algorithm, when selecting a scaling technique.\\n- Evaluate the impact of scaling on the performance and interpretability of machine learning models, and consider alternative scaling methods if necessary.\\n- Document the scaling process and any assumptions made to ensure transparency and reproducibility in data analysis workflows.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["from sklearn.impute import SimpleImputer\n","# Identify numerical and categorical columns from the DataFrame\n","\n","numerical_columns = df_withoutiqroutliers_scaled.select_dtypes(include=['float64', 'int64']).columns\n","categorical_columns = df_withoutiqroutliers_scaled.select_dtypes(include=['object']).columns\n","\n","# Method 1: Mean Imputation (for numerical columns)\n","imputer_mean = SimpleImputer(strategy='mean')\n","df_scaled_mean_imputed = df_withoutiqroutliers_scaled.copy()\n","df_scaled_mean_imputed[numerical_columns] = imputer_mean.fit_transform(df_withoutiqroutliers_scaled[numerical_columns])\n","\n","# Method 2: Median Imputation (for numerical columns)\n","imputer_median = SimpleImputer(strategy='median')\n","df_scaled_median_imputed = df_withoutiqroutliers_scaled.copy()\n","df_scaled_median_imputed[numerical_columns] = imputer_median.fit_transform(df_withoutiqroutliers_scaled[numerical_columns])\n","\n","# Method 3: Most Frequent Imputation (for non-numerical columns)\n","imputer_mode = SimpleImputer(strategy='most_frequent')\n","df_scaled_mode_imputed = df_withoutiqroutliers_scaled.copy()\n","df_scaled_mode_imputed[categorical_columns] = imputer_mode.fit_transform(df_withoutiqroutliers_scaled[categorical_columns])\n","\n","# Remove rows containing NaN values for the entire DataFrame\n","df_scaled_mean_imputed_dropna = df_scaled_mean_imputed.dropna()\n","df_scaled_median_imputed_dropna = df_scaled_median_imputed.dropna()\n","df_scaled_mode_imputed_dropna = df_scaled_mode_imputed.dropna()\n","\n","# Remove rows containing NaN values for a particular column (example: 'Income')\n","column_to_dropna = 'Income'\n","df_scaled_mean_imputed_dropna_column = df_scaled_mean_imputed.dropna(subset=[column_to_dropna])\n","df_scaled_median_imputed_dropna_column = df_scaled_median_imputed.dropna(subset=[column_to_dropna])\n","df_scaled_mode_imputed_dropna_column = df_scaled_mode_imputed.dropna(subset=[column_to_dropna])\n","\n","print(\"Mean Imputation (Numerical) with NaN Removal:\")\n","print(df_scaled_mean_imputed_dropna)\n","\n","print(\"\\nMedian Imputation (Numerical) with NaN Removal:\")\n","print(df_scaled_median_imputed_dropna)\n","\n","print(\"\\nMost Frequent Imputation (Categorical) with NaN Removal:\")\n","print(df_scaled_mode_imputed_dropna)\n","\n","print(\"\\nMean Imputation (Numerical) with NaN Removal for Column '{}':\".format(column_to_dropna))\n","print(df_scaled_mean_imputed_dropna_column)\n","\n","print(\"\\nMedian Imputation (Numerical) with NaN Removal for Column '{}':\".format(column_to_dropna))\n","print(df_scaled_median_imputed_dropna_column)\n","\n","print(\"\\nMost Frequent Imputation (Categorical) with NaN Removal for Column '{}':\".format(column_to_dropna))\n","print(df_scaled_mode_imputed_dropna_column)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vrqi5Iu9-y5e","executionInfo":{"status":"ok","timestamp":1712471177357,"user_tz":0,"elapsed":343,"user":{"displayName":"Márk Németh","userId":"08804061133513418249"}},"outputId":"2238b76b-b9e4-4499-8d8a-7882955a6a63"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Imputation (Numerical) with NaN Removal:\n","    ID   Name       Age  Gender    Income  Education  Score Label Has_Car  \\\n","0  1.0   John  0.000000     1.0  0.000000        0.0  0.875   Yes    True   \n","3  4.0   Mary  1.000000     0.0  0.666667        3.0  0.000   Yes    True   \n","4  5.0   Jane  0.666667     0.0  1.000000        0.0  0.750    No    True   \n","5  1.0   John  0.000000     1.0  0.000000        0.0  0.875   Yes    True   \n","6  2.0  Alice  0.333333     0.0  0.333333        2.0  1.000    No   False   \n","\n","   Location  High_Income  \n","0       0.0        False  \n","3       0.0         True  \n","4       2.0         True  \n","5       0.0        False  \n","6       2.0        False  \n","\n","Median Imputation (Numerical) with NaN Removal:\n","    ID   Name       Age  Gender    Income  Education  Score Label Has_Car  \\\n","0  1.0   John  0.000000     1.0  0.000000        0.0  0.875   Yes    True   \n","3  4.0   Mary  1.000000     0.0  0.666667        3.0  0.000   Yes    True   \n","4  5.0   Jane  0.666667     0.0  1.000000        0.0  0.750    No    True   \n","5  1.0   John  0.000000     1.0  0.000000        0.0  0.875   Yes    True   \n","6  2.0  Alice  0.333333     0.0  0.333333        2.0  1.000    No   False   \n","\n","   Location  High_Income  \n","0       0.0        False  \n","3       0.0         True  \n","4       2.0         True  \n","5       0.0        False  \n","6       2.0        False  \n","\n","Most Frequent Imputation (Categorical) with NaN Removal:\n","   ID   Name       Age  Gender    Income  Education  Score Label Has_Car  \\\n","0   1   John  0.000000       1  0.000000          0  0.875   Yes    True   \n","1   2  Alice  0.333333       0  0.333333          2  1.000    No    True   \n","3   4   Mary  1.000000       0  0.666667          3  0.000   Yes    True   \n","4   5   Jane  0.666667       0  1.000000          0  0.750    No    True   \n","5   1   John  0.000000       1  0.000000          0  0.875   Yes    True   \n","6   2  Alice  0.333333       0  0.333333          2  1.000    No   False   \n","\n","   Location  High_Income  \n","0         0        False  \n","1         2        False  \n","3         0         True  \n","4         2         True  \n","5         0        False  \n","6         2        False  \n","\n","Mean Imputation (Numerical) with NaN Removal for Column 'Income':\n","    ID   Name       Age  Gender    Income  Education  Score Label Has_Car  \\\n","0  1.0   John  0.000000     1.0  0.000000        0.0  0.875   Yes    True   \n","1  2.0  Alice  0.333333     0.0  0.333333        2.0  1.000    No     NaN   \n","3  4.0   Mary  1.000000     0.0  0.666667        3.0  0.000   Yes    True   \n","4  5.0   Jane  0.666667     0.0  1.000000        0.0  0.750    No    True   \n","5  1.0   John  0.000000     1.0  0.000000        0.0  0.875   Yes    True   \n","6  2.0  Alice  0.333333     0.0  0.333333        2.0  1.000    No   False   \n","\n","   Location  High_Income  \n","0       0.0        False  \n","1       2.0        False  \n","3       0.0         True  \n","4       2.0         True  \n","5       0.0        False  \n","6       2.0        False  \n","\n","Median Imputation (Numerical) with NaN Removal for Column 'Income':\n","    ID   Name       Age  Gender    Income  Education  Score Label Has_Car  \\\n","0  1.0   John  0.000000     1.0  0.000000        0.0  0.875   Yes    True   \n","1  2.0  Alice  0.333333     0.0  0.333333        2.0  1.000    No     NaN   \n","3  4.0   Mary  1.000000     0.0  0.666667        3.0  0.000   Yes    True   \n","4  5.0   Jane  0.666667     0.0  1.000000        0.0  0.750    No    True   \n","5  1.0   John  0.000000     1.0  0.000000        0.0  0.875   Yes    True   \n","6  2.0  Alice  0.333333     0.0  0.333333        2.0  1.000    No   False   \n","\n","   Location  High_Income  \n","0       0.0        False  \n","1       2.0        False  \n","3       0.0         True  \n","4       2.0         True  \n","5       0.0        False  \n","6       2.0        False  \n","\n","Most Frequent Imputation (Categorical) with NaN Removal for Column 'Income':\n","   ID   Name       Age  Gender    Income  Education  Score Label Has_Car  \\\n","0   1   John  0.000000       1  0.000000          0  0.875   Yes    True   \n","1   2  Alice  0.333333       0  0.333333          2  1.000    No    True   \n","3   4   Mary  1.000000       0  0.666667          3  0.000   Yes    True   \n","4   5   Jane  0.666667       0  1.000000          0  0.750    No    True   \n","5   1   John  0.000000       1  0.000000          0  0.875   Yes    True   \n","6   2  Alice  0.333333       0  0.333333          2  1.000    No   False   \n","\n","   Location  High_Income  \n","0         0        False  \n","1         2        False  \n","3         0         True  \n","4         2         True  \n","5         0        False  \n","6         2        False  \n"]}]},{"cell_type":"code","source":["from sklearn.feature_selection import SelectKBest, f_regression, RFE, SelectFromModel\n","from sklearn.linear_model import LinearRegression\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# Define numerical columns\n","numerical_columns = df_scaled_mean_imputed_dropna.select_dtypes(include=['float64', 'int64']).columns\n","\n","# Define categorical columns\n","categorical_columns = df_scaled_mean_imputed_dropna.select_dtypes(include=['object']).columns\n","\n","# Define the target variable\n","target_variable = 'High_Income'\n","\n","# Method 1: SelectKBest with f_regression (for numerical columns)\n","selector_kbest = SelectKBest(score_func=f_regression, k=2)\n","X_kbest = selector_kbest.fit_transform(df_scaled_mean_imputed_dropna[numerical_columns], df_scaled_mean_imputed_dropna[target_variable])\n","\n","# Method 2: Recursive Feature Elimination (RFE) with Linear Regression (for numerical columns)\n","estimator = LinearRegression()\n","selector_rfe = RFE(estimator, n_features_to_select=2, step=1)\n","X_rfe = selector_rfe.fit_transform(df_scaled_mean_imputed_dropna[numerical_columns], df_scaled_mean_imputed_dropna[target_variable])\n","\n","# Print selected features for SelectKBest\n","selected_indices_kbest = selector_kbest.get_support(indices=True)\n","selected_features_kbest = df_scaled_mean_imputed_dropna.columns[selected_indices_kbest]\n","print(\"Selected features for SelectKBest:\", selected_features_kbest)\n","\n","\n","\n","\"\"\"\n","Feature Selection Methods:\n","\n","1. SelectKBest (SelectKBest):\n","   - Summary: Selects the top k features based on their scores using a specified scoring function.\n","   - Benefits:\n","     - Directly specifies the desired number of features to select.\n","     - Computes feature scores independently, making it efficient for large datasets.\n","   - Use Cases:\n","     - Suitable for datasets with many features where a predefined number of features is desired.\n","     - Useful for feature engineering and dimensionality reduction in machine learning pipelines.\n","\n","2. Recursive Feature Elimination (RFE):\n","   - Summary: Recursively removes features based on their importance until the specified number of features is reached.\n","   - Benefits:\n","     - Considers feature interactions and their collective contribution to the model's performance.\n","     - Provides more robust feature selection by iteratively evaluating feature importance.\n","   - Use Cases:\n","     - Effective for datasets with a moderate number of features and complex relationships between features.\n","     - Suitable for improving model interpretability and reducing overfitting by selecting relevant features.\n","\n","3. SelectFromModel (SelectFromModel):\n","   - Summary: Selects features based on their importance provided by a specified model.\n","   - Benefits:\n","     - Utilizes the feature importances provided by the model to select relevant features.\n","     - Can handle both numerical and categorical features, providing flexibility in feature selection.\n","   - Use Cases:\n","     - Useful for datasets with mixed data types or when feature importance needs to be assessed comprehensively.\n","     - Effective for identifying important features in ensemble models such as random forests or gradient boosting machines.\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"kLu_9n1GCxvJ","executionInfo":{"status":"ok","timestamp":1712472145841,"user_tz":0,"elapsed":418,"user":{"displayName":"Márk Németh","userId":"08804061133513418249"}},"outputId":"b2e6e51f-93d0-4f86-9af3-3240d511089f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Selected features for SelectKBest: Index(['ID', 'Name'], dtype='object')\n"]},{"output_type":"execute_result","data":{"text/plain":["\"\\nFeature Selection Methods:\\n\\n1. SelectKBest (SelectKBest):\\n   - Summary: Selects the top k features based on their scores using a specified scoring function.\\n   - Benefits:\\n     - Directly specifies the desired number of features to select.\\n     - Computes feature scores independently, making it efficient for large datasets.\\n   - Use Cases:\\n     - Suitable for datasets with many features where a predefined number of features is desired.\\n     - Useful for feature engineering and dimensionality reduction in machine learning pipelines.\\n\\n2. Recursive Feature Elimination (RFE):\\n   - Summary: Recursively removes features based on their importance until the specified number of features is reached.\\n   - Benefits:\\n     - Considers feature interactions and their collective contribution to the model's performance.\\n     - Provides more robust feature selection by iteratively evaluating feature importance.\\n   - Use Cases:\\n     - Effective for datasets with a moderate number of features and complex relationships between features.\\n     - Suitable for improving model interpretability and reducing overfitting by selecting relevant features.\\n\\n3. SelectFromModel (SelectFromModel):\\n   - Summary: Selects features based on their importance provided by a specified model.\\n   - Benefits:\\n     - Utilizes the feature importances provided by the model to select relevant features.\\n     - Can handle both numerical and categorical features, providing flexibility in feature selection.\\n   - Use Cases:\\n     - Useful for datasets with mixed data types or when feature importance needs to be assessed comprehensively.\\n     - Effective for identifying important features in ensemble models such as random forests or gradient boosting machines.\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["# Convert categorical variables to numerical using one-hot encoding\n","df_encoded = pd.get_dummies(df_scaled_mean_imputed_dropna)\n","\n","# Apply PCA to the encoded DataFrame\n","pca = PCA(n_components=2)  # Specify the number of principal components\n","X_pca = pca.fit_transform(df_encoded)\n","\n","# Print explained variance ratio\n","print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n","\n","\n","\"\"\"\n","Principal Component Analysis (PCA):\n","\n","- Summary:\n","  PCA is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while preserving the maximum variance in the data. It identifies the principal components, which are orthogonal vectors that represent the directions of maximum variance in the original feature space.\n","\n","- What is Calculated:\n","  PCA calculates the eigenvectors and eigenvalues of the covariance matrix of the input data. The eigenvectors (principal components) represent the directions of maximum variance, while the eigenvalues represent the amount of variance explained by each principal component.\n","\n","- How Can it be Used:\n","  1. Dimensionality Reduction: PCA is used to reduce the dimensionality of data by projecting it onto a lower-dimensional subspace while retaining most of the important information.\n","  2. Visualization: PCA is often used for data visualization by transforming high-dimensional data into a 2D or 3D space, making it easier to visualize and interpret.\n","  3. Noise Reduction: PCA can help in reducing noise and removing redundant features from the data, leading to improved model performance.\n","\n","- Alternatives:\n","  1. Linear Discriminant Analysis (LDA): LDA is another dimensionality reduction technique that takes into account class labels and aims to maximize the separation between classes.\n","  2. t-distributed Stochastic Neighbor Embedding (t-SNE): t-SNE is a nonlinear dimensionality reduction technique that is particularly effective for visualizing high-dimensional data in low-dimensional space while preserving local structures.\n","\n","- Benefits:\n","  1. Reduces Dimensionality: PCA reduces the number of features in the data while retaining most of the variance, making it computationally efficient and improving model performance.\n","  2. Removes Redundancy: PCA identifies and removes redundant features, leading to simpler and more interpretable models.\n","  3. Visualization: PCA helps in visualizing high-dimensional data in lower-dimensional space, facilitating data exploration and interpretation.\n","\n","- Use Cases:\n","  1. Image Compression: PCA is used in image compression to reduce the size of images while preserving important features.\n","  2. Gene Expression Analysis: PCA is used in genomics to identify patterns and reduce noise in gene expression data.\n","  3. Financial Analysis: PCA is used in finance for portfolio optimization and risk management by identifying the principal components of asset returns.\n","\n","\"\"\"\n","\n","\n","\"\"\"\n","Principal Component Analysis (PCA):\n","\n","PCA is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while preserving the maximum variance in the data. It identifies the principal components, which are orthogonal vectors that represent the directions of maximum variance in the original feature space.\n","\n","Parameters:\n","    - n_components (int or None, default=None): Number of components to keep. If None, all components are kept.\n","\n","Attributes:\n","    - components_ (ndarray of shape (n_components, n_features)): Principal axes in feature space, representing the directions of maximum variance.\n","    - explained_variance_ratio_ (ndarray of shape (n_components,)): Percentage of variance explained by each of the selected components.\n","\n","Methods:\n","    - fit(X, y=None): Fit the PCA model to the data.\n","    - transform(X): Apply dimensionality reduction to X.\n","    - fit_transform(X, y=None): Fit the PCA model to the data and transform X.\n","\n","Use Case:\n","PCA is commonly used for dimensionality reduction, visualization, and noise reduction in high-dimensional datasets. It finds application in various domains such as image processing, genetics, finance, and more.\n","\n","Benefits:\n","1. Dimensionality Reduction: PCA reduces the number of features in the data while retaining most of the variance, making it computationally efficient and improving model performance.\n","2. Removes Redundancy: PCA identifies and removes redundant features, leading to simpler and more interpretable models.\n","3. Visualization: PCA helps in visualizing high-dimensional data in lower-dimensional space, facilitating data exploration and interpretation.\n","4. Noise Reduction: PCA can help in reducing noise and removing redundant features from the data, leading to improved model performance.\n","\n","Walkthrough:\n","1. Import the PCA class from the sklearn.decomposition module.\n","2. Create a PCA object with the desired number of components.\n","3. Fit the PCA object to the input data using the fit method.\n","4. Transform the input data into the lower-dimensional space using the transform method.\n","5. Optionally, compute and analyze the explained variance ratio to understand the contribution of each principal component to the total variance.\n","\n","Example:\n","# Apply PCA to the input DataFrame\n","pca = PCA(n_components=2)  # Specify the number of principal components\n","X_pca = pca.fit_transform(df_scaled_mean_imputed_dropna)\n","\n","# Print explained variance ratio\n","print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n","\n","\"\"\"\n","\n","\n","\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","\n","# Determine the minimum between the number of features and the number of classes minus one\n","min_components = min(X.shape[1], len(np.unique(y)) - 1)\n","\n","# Apply Linear Discriminant Analysis (LDA) with the appropriate number of components\n","lda = LinearDiscriminantAnalysis(n_components=min_components)\n","X_lda = lda.fit_transform(X, y)\n","\n","# Print explained variance ratio\n","print(\"Explained variance ratio:\", lda.explained_variance_ratio_)\n","\n","\n","\"\"\"\n","Linear Discriminant Analysis (LDA) Alternative:\n","\n","Apply Linear Discriminant Analysis (LDA) as an alternative dimensionality reduction technique to Principal Component Analysis (PCA) on the provided input DataFrame.\n","\n","Parameters:\n","-----------\n","df_scaled_mean_imputed_dropna : pandas DataFrame\n","    Input DataFrame after mean imputation and removal of rows with NaN values.\n","\n","Returns:\n","--------\n","X_lda : array-like, shape (n_samples, n_components)\n","    Reduced-dimensional data after applying Linear Discriminant Analysis (LDA).\n","\n","Explanation:\n","------------\n","Linear Discriminant Analysis (LDA) is a dimensionality reduction technique that takes into account class labels to find the linear combinations of features that best separate different classes in the data.\n","\n","Use Case:\n","---------\n","LDA is commonly used in classification tasks where maximizing class separability is important. It's particularly useful when there are multiple classes in the dataset and the goal is to reduce the dimensionality while preserving class-related information.\n","\n","Benefits:\n","---------\n","- LDA takes into account class labels, making it effective for classification tasks.\n","- It maximizes class separability in the reduced-dimensional space, leading to better classification performance.\n","- LDA provides insights into the discriminatory power of different features, helping in feature selection and model interpretation.\n","\n","Walkthrough:\n","------------\n","1. Load the data and separate features and the target variable.\n","2. Apply LDA to the features with the target variable.\n","3. Use the reduced-dimensional data for classification or visualization tasks.\n","\n","By applying LDA, you can reduce the dimensionality of your data while preserving class-related information, which can be beneficial for various classificat\n","\n","\"\"\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"id":"miEmasSVBq4q","executionInfo":{"status":"ok","timestamp":1712473931097,"user_tz":0,"elapsed":306,"user":{"displayName":"Márk Németh","userId":"08804061133513418249"}},"outputId":"465a5bc8-08d5-41f8-d07a-5f7d6bd39b67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Explained variance ratio: [0.56009292 0.24328888]\n","Explained variance ratio: [1.]\n"]},{"output_type":"execute_result","data":{"text/plain":["\"\\nLinear Discriminant Analysis (LDA) Alternative:\\n\\nApply Linear Discriminant Analysis (LDA) as an alternative dimensionality reduction technique to Principal Component Analysis (PCA) on the provided input DataFrame.\\n\\nParameters:\\n-----------\\ndf_scaled_mean_imputed_dropna : pandas DataFrame\\n    Input DataFrame after mean imputation and removal of rows with NaN values.\\n\\nReturns:\\n--------\\nX_lda : array-like, shape (n_samples, n_components)\\n    Reduced-dimensional data after applying Linear Discriminant Analysis (LDA).\\n\\nExplanation:\\n------------\\nLinear Discriminant Analysis (LDA) is a dimensionality reduction technique that takes into account class labels to find the linear combinations of features that best separate different classes in the data.\\n\\nUse Case:\\n---------\\nLDA is commonly used in classification tasks where maximizing class separability is important. It's particularly useful when there are multiple classes in the dataset and the goal is to reduce the dimensionality while preserving class-related information.\\n\\nBenefits:\\n---------\\n- LDA takes into account class labels, making it effective for classification tasks.\\n- It maximizes class separability in the reduced-dimensional space, leading to better classification performance.\\n- LDA provides insights into the discriminatory power of different features, helping in feature selection and model interpretation.\\n\\nWalkthrough:\\n------------\\n1. Load the data and separate features and the target variable.\\n2. Apply LDA to the features with the target variable.\\n3. Use the reduced-dimensional data for classification or visualization tasks.\\n\\nBy applying LDA, you can reduce the dimensionality of your data while preserving class-related information, which can be beneficial for various classificat\\n\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["# Load your dataframe (df_scaled_mean_imputed_dropna) here\n","\n","# Method 1: Random Noise Addition\n","def add_noise(dataframe, noise_level=0.1):\n","    \"\"\"\n","    Randomly adds noise to numerical features in the DataFrame.\n","\n","    Parameters:\n","    - dataframe: Original DataFrame\n","    - noise_level: Level of noise to be added (default: 0.1)\n","\n","    Returns:\n","    - Augmented DataFrame with added noise\n","\n","    Explanation:\n","    Random noise addition is a simple data augmentation technique used to introduce variability into the dataset by adding random noise to numerical features. This can help in making the model more robust and generalize better to unseen data.\n","\n","    Benefits:\n","    - Increases Robustness: Adding noise helps in making the model more robust by reducing overfitting and capturing underlying patterns in the data.\n","    - Improves Generalization: Introducing variability through noise encourages the model to learn more generalized patterns, leading to better performance on unseen data.\n","\n","    Alternatives:\n","    - Gaussian Noise Injection: Similar to random noise addition, Gaussian noise injection adds noise to the dataset using a Gaussian distribution with specified mean and standard deviation.\n","    - Uniform Noise Injection: Uniform noise injection adds noise using a uniform distribution within a specified range.\n","\n","    Use Cases:\n","    - Classification and Regression Tasks: Random noise addition is commonly used in classification and regression tasks where the goal is to improve model performance and generalization.\n","    - Image Processing: In image processing tasks, adding random noise can help in augmenting the dataset for tasks like denoising and image reconstruction.\n","\n","    Walkthrough:\n","    1. Select Numerical Features: Identify numerical features in the dataset.\n","    2. Add Noise: Generate random noise with a specified level and add it to the numerical features.\n","    3. Return Augmented DataFrame: Return the augmented DataFrame with added noise.\n","\n","    Example:\n","    >>> # Apply Random Noise Addition\n","    >>> df_noisy = add_noise(df_scaled_mean_imputed_dropna)\n","    >>> # Print the first few rows of the augmented DataFrame\n","    >>> print(\"Augmented DataFrame with Random Noise:\")\n","    >>> print(df_noisy.head())\n","    \"\"\"\n","    noisy_dataframe = dataframe.copy()\n","    numerical_columns = noisy_dataframe.select_dtypes(include=['float64', 'int64']).columns\n","    for column in numerical_columns:\n","        noise = np.random.normal(0, noise_level, len(noisy_dataframe))\n","        noisy_dataframe[column] += noise\n","    return noisy_dataframe\n","\n","# Data Augmentation Techniques:\n","\n","# Method 2: Shuffle Data\n","def shuffle_data(dataframe):\n","    \"\"\"\n","    Shuffles the rows of the DataFrame to introduce randomness.\n","\n","    Parameters:\n","    - dataframe: Original DataFrame\n","\n","    Returns:\n","    - Augmented DataFrame with shuffled rows\n","\n","    Explanation:\n","    Shuffling the data involves randomly reordering the rows of the DataFrame. This technique helps in introducing randomness into the dataset, which can prevent the model from memorizing the order of the data and improve its generalization ability.\n","\n","    Benefits:\n","    - Prevents Overfitting: Shuffling the data prevents the model from memorizing patterns based on the order of the data, thus reducing overfitting.\n","    - Increases Robustness: Introducing randomness through shuffling makes the model more robust by exposing it to a variety of data configurations.\n","\n","    Alternatives:\n","    - Random Sampling: Randomly selecting a subset of data points from the dataset can also introduce randomness, but it may not shuffle the entire dataset.\n","    - Time-Based Shuffling: Shuffling data based on timestamps or time intervals can be useful for time-series data.\n","\n","    Use Cases:\n","    - Time-Series Forecasting: Shuffling data can be beneficial in time-series forecasting tasks to prevent the model from learning patterns based on the sequence of timestamps.\n","    - Text Classification: In natural language processing tasks, shuffling data can help in text classification tasks by preventing the model from learning patterns based on the order of documents.\n","\n","    Walkthrough:\n","    1. Shuffle Rows: Randomly shuffle the rows of the DataFrame to introduce randomness.\n","    2. Return Augmented DataFrame: Return the DataFrame with shuffled rows.\n","\n","    Example:\n","    >>> # Apply Shuffle Data\n","    >>> df_shuffled = shuffle_data(df_scaled_mean_imputed_dropna)\n","    >>> # Print the first few rows of the augmented DataFrame\n","    >>> print(\"Augmented DataFrame with Shuffled Data:\")\n","    >>> print(df_shuffled.head())\n","    \"\"\"\n","    shuffled_dataframe = shuffle(dataframe)\n","    return shuffled_dataframe\n","\n","def create_duplicates(dataframe, num_duplicates=1):\n","    \"\"\"\n","    Creates duplicates of the original DataFrame.\n","\n","    Parameters:\n","    - dataframe: Original DataFrame\n","    - num_duplicates: Number of duplicates to create (default: 1)\n","\n","    Returns:\n","    - Augmented DataFrame with duplicates\n","\n","    Explanation:\n","    Duplicate creation is a simple data augmentation technique that involves replicating existing samples in the dataset. By creating duplicates, we can increase the size of the dataset, which can be beneficial for training robust models, especially when the original dataset is small.\n","\n","    Benefits:\n","    - Increased Dataset Size: Duplicate creation expands the dataset size, providing more data for training and improving the model's ability to generalize.\n","    - Preserve Original Distribution: Duplicates maintain the distribution of the original dataset, ensuring that the characteristics of the data are preserved.\n","\n","    Alternatives:\n","    - Bootstrapping: Bootstrapping is a resampling technique that involves generating new samples by randomly sampling with replacement from the original dataset.\n","    - Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a method used for imbalanced datasets that generates synthetic samples for minority classes by interpolating between existing samples.\n","\n","    Use Cases:\n","    - Imbalanced Datasets: Duplicate creation can be useful for balancing class distributions in imbalanced datasets, where one class is underrepresented.\n","    - Small Datasets: Duplicate creation can help in augmenting small datasets to provide more training data for machine learning models.\n","\n","    Walkthrough:\n","    1. Specify Number of Duplicates: Determine the number of duplicates to create.\n","    2. Create Duplicates: Replicate existing samples in the dataset based on the specified number of duplicates.\n","    3. Return Augmented DataFrame: Return the augmented DataFrame with duplicates.\n","\n","    Example:\n","    >>> # Apply Duplicate Creation\n","    >>> df_duplicates = create_duplicates(df_scaled_mean_imputed_dropna, num_duplicates=3)\n","    >>> # Print the number of samples in the augmented DataFrame\n","    >>> print(\"Number of Samples after Duplicate Creation:\", len(df_duplicates))\n","    \"\"\"\n","    duplicated_dataframe = pd.concat([dataframe] * num_duplicates, ignore_index=True)\n","    return duplicated_dataframe\n","\n","\n","# Data Augmentation Techniques:\n","\n","# Method 2: Scale Features\n","def scale_features(dataframe, scale_factor=0.1):\n","    \"\"\"\n","    Scales numerical features in the DataFrame by a specified factor.\n","\n","    Parameters:\n","    - dataframe: Original DataFrame\n","    - scale_factor: Factor by which to scale the features (default: 0.1)\n","\n","    Returns:\n","    - Augmented DataFrame with scaled features\n","\n","    Explanation:\n","    Scaling features involves multiplying numerical features by a specified scale factor, which can help in augmenting the dataset by adjusting the magnitude of the features. This can be useful for tasks where feature magnitudes play a significant role in model performance.\n","\n","    Benefits:\n","    - Adjusts Feature Magnitudes: Scaling features allows adjusting the magnitudes of numerical features, which can improve model convergence and stability.\n","    - Encourages Feature Importance: By scaling features, the importance of different features can be emphasized or de-emphasized based on their magnitudes.\n","\n","    Alternatives:\n","    - Min-Max Scaling: Min-Max scaling scales features to a specified range (e.g., [0, 1]) by subtracting the minimum value and dividing by the range of values.\n","    - Standardization: Standardization scales features to have a mean of 0 and a standard deviation of 1 by subtracting the mean and dividing by the standard deviation.\n","\n","    Use Cases:\n","    - Gradient-Based Optimization: Scaling features is commonly used in gradient-based optimization algorithms such as gradient descent to ensure uniform updates across features.\n","    - Neural Network Training: In neural network training, scaling features can help in improving the convergence of the training process and avoiding vanishing or exploding gradients.\n","\n","    Walkthrough:\n","    1. Select Numerical Features: Identify numerical features in the dataset.\n","    2. Scale Features: Multiply the numerical features by the specified scale factor.\n","    3. Return Augmented DataFrame: Return the augmented DataFrame with scaled features.\n","\n","    Example:\n","    >>> # Apply Scale Features\n","    >>> df_scaled = scale_features(df_scaled_mean_imputed_dropna)\n","    >>> # Print the first few rows of the augmented DataFrame\n","    >>> print(\"Augmented DataFrame with Scaled Features:\")\n","    >>> print(df_scaled.head())\n","    \"\"\"\n","    scaled_dataframe = dataframe.copy()\n","    numerical_columns = scaled_dataframe.select_dtypes(include=['float64', 'int64']).columns\n","    scaled_dataframe[numerical_columns] *= scale_factor\n","    return scaled_dataframe\n","\n","\n","# Data Augmentation Techniques:\n","\n","# Data Augmentation Techniques:\n","\n","# Method 2: Synthetic Minority Over-sampling Technique (SMOTE)\n","def apply_smote(dataframe, target_column, k_neighbors=1):\n","    \"\"\"\n","    Applies Synthetic Minority Over-sampling Technique (SMOTE) to balance classes in the DataFrame.\n","\n","    Parameters:\n","    - dataframe: Original DataFrame\n","    - target_column: Name of the target column containing class labels\n","\n","    Returns:\n","    - Augmented DataFrame with balanced classes using SMOTE\n","\n","    Explanation:\n","    Synthetic Minority Over-sampling Technique (SMOTE) is a data augmentation technique used to address class imbalance by generating synthetic samples for the minority class. It works by interpolating between existing minority class samples to create new synthetic samples.\n","\n","    Benefits:\n","    - Addresses Class Imbalance: SMOTE helps in balancing class distribution by generating synthetic samples for the minority class, which can improve model performance and reduce bias.\n","    - Preserves Information: SMOTE generates synthetic samples by considering the local structure of the data, preserving important information and reducing the risk of overfitting.\n","\n","    Alternatives:\n","    - ADASYN (Adaptive Synthetic Sampling): ADASYN is an extension of SMOTE that adaptively generates synthetic samples based on the density distribution of the minority class, focusing on areas of higher density.\n","    - Borderline-SMOTE: Borderline-SMOTE is a variant of SMOTE that focuses on generating synthetic samples near the decision boundary between classes to improve classification performance.\n","\n","    Use Cases:\n","    - Classification Tasks with Imbalanced Classes: SMOTE is commonly used in classification tasks where class imbalance is prevalent, such as fraud detection, medical diagnosis, and anomaly detection.\n","    - Machine Learning Pipelines: SMOTE can be integrated into machine learning pipelines to preprocess data before model training, improving model performance and generalization.\n","\n","    Walkthrough:\n","    1. Preprocess Data: Encode categorical variables and handle missing values.\n","    2. Identify Minority Class: Determine the minority class based on the target column.\n","    3. Apply SMOTE: Apply SMOTE to generate synthetic samples for the minority class, balancing class distribution.\n","    4. Return Augmented DataFrame: Return the augmented DataFrame with balanced classes using SMOTE.\n","\n","    Example:\n","    >>> # Apply SMOTE\n","    >>> augmented_df_smote = apply_smote(df_scaled_mean_imputed_dropna, 'High_Income')\n","    >>> # Print the augmented DataFrame with SMOTE\n","    >>> print(\"Augmented DataFrame with SMOTE:\")\n","    >>> print(augmented_df_smote)\n","    \"\"\"\n","    # Preprocess data: Encode categorical variables and handle missing values\n","    dataframe_encoded = pd.get_dummies(dataframe, columns=dataframe.select_dtypes(include=['object']).columns)\n","    dataframe_encoded.dropna(inplace=True)\n","\n","    # Apply SMOTE to balance classes\n","    smote = SMOTE(k_neighbors=k_neighbors)\n","    X_resampled, y_resampled = smote.fit_resample(dataframe_encoded.drop(columns=[target_column]), dataframe_encoded[target_column])\n","\n","    # Combine resampled features and target variable into a DataFrame\n","    augmented_df = pd.concat([pd.DataFrame(X_resampled, columns=dataframe_encoded.drop(columns=[target_column]).columns), pd.Series(y_resampled, name=target_column)], axis=1)\n","\n","    return augmented_df\n","\n","\n","\n","# Apply Random Noise Addition\n","df_noisy = add_noise(df_scaled_mean_imputed_dropna)\n","\n","# Print the first few rows of the augmented DataFrame\n","print(\"Augmented DataFrame with Random Noise:\")\n","print(df_noisy.head())\n","\n","# Apply Shuffle Data\n","df_shuffled = shuffle_data(df_scaled_mean_imputed_dropna)\n","\n","# Print the first few rows of the augmented DataFrame\n","print(\"Augmented DataFrame with Shuffled Data:\")\n","print(df_shuffled.head())\n","\n","\n","# Apply Duplicate Creation\n","df_duplicates = create_duplicates(df_scaled_mean_imputed_dropna, num_duplicates=3)\n","\n","# Print the number of samples in the augmented DataFrame\n","print(\"Number of Samples after Duplicate Creation:\", len(df_duplicates))\n","\n","\n","\n","# Apply Scale Features\n","df_scaled = scale_features(df_scaled_mean_imputed_dropna)\n","\n","# Print the first few rows of the augmented DataFrame\n","print(\"Augmented DataFrame with Scaled Features:\")\n","print(df_scaled.head())\n","\n","\n","# Example of Data Augmentation with Concatenation:\n","\n","# Augment the data using different techniques\n","augmented_df_noise = add_noise(df_scaled_mean_imputed_dropna)\n","augmented_df_shuffle = shuffle_data(df_scaled_mean_imputed_dropna)\n","augmented_df_duplicate = create_duplicates(df_scaled_mean_imputed_dropna)\n","augmented_df_scale = scale_features(df_scaled_mean_imputed_dropna)\n","\n","# Concatenate the augmented data with the original data\n","concatenated_df = pd.concat([df_scaled_mean_imputed_dropna, augmented_df_noise, augmented_df_shuffle, augmented_df_duplicate, augmented_df_scale], ignore_index=True)\n","\n","# Print the concatenated DataFrame\n","print(\"\\nConcatenated DataFrame:\")\n","print(concatenated_df.head())\n","\n","# Apply SMOTE\n","augmented_df_smote = apply_smote(df_scaled_mean_imputed_dropna, 'High_Income')\n","\n","# Print the augmented DataFrame with SMOTE\n","print(\"Augmented DataFrame with SMOTE:\")\n","print(augmented_df_smote.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3TRxpu4PX9-D","executionInfo":{"status":"ok","timestamp":1712475261025,"user_tz":0,"elapsed":359,"user":{"displayName":"Márk Németh","userId":"08804061133513418249"}},"outputId":"63859c17-9087-4883-da93-1438ecea98fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Augmented DataFrame with Random Noise:\n","         ID   Name       Age    Gender    Income  Education     Score Label  \\\n","0  1.002483   John -0.036056  1.043741 -0.003362  -0.233972  0.944378   Yes   \n","3  4.032752   Mary  0.796761  0.086634  0.602848   2.936484 -0.020696   Yes   \n","4  4.950992   Jane  0.678570  0.026424  1.060908  -0.192964  0.806892    No   \n","5  0.793612   John -0.029316  1.055142  0.160090  -0.035306  0.804280   Yes   \n","6  1.968148  Alice  0.383258  0.059591  0.496383   1.814357  1.083006    No   \n","\n","  Has_Car  Location  High_Income  \n","0    True  0.012616        False  \n","3    True -0.013245         True  \n","4    True  1.868057         True  \n","5    True -0.070378        False  \n","6   False  1.985968        False  \n","Augmented DataFrame with Shuffled Data:\n","    ID   Name       Age  Gender    Income  Education  Score Label Has_Car  \\\n","0  1.0   John  0.000000     1.0  0.000000        0.0  0.875   Yes    True   \n","3  4.0   Mary  1.000000     0.0  0.666667        3.0  0.000   Yes    True   \n","5  1.0   John  0.000000     1.0  0.000000        0.0  0.875   Yes    True   \n","4  5.0   Jane  0.666667     0.0  1.000000        0.0  0.750    No    True   \n","6  2.0  Alice  0.333333     0.0  0.333333        2.0  1.000    No   False   \n","\n","   Location  High_Income  \n","0       0.0        False  \n","3       0.0         True  \n","5       0.0        False  \n","4       2.0         True  \n","6       2.0        False  \n","Number of Samples after Duplicate Creation: 15\n","Augmented DataFrame with Scaled Features:\n","    ID   Name       Age  Gender    Income  Education   Score Label Has_Car  \\\n","0  0.1   John  0.000000     0.1  0.000000        0.0  0.0875   Yes    True   \n","3  0.4   Mary  0.100000     0.0  0.066667        0.3  0.0000   Yes    True   \n","4  0.5   Jane  0.066667     0.0  0.100000        0.0  0.0750    No    True   \n","5  0.1   John  0.000000     0.1  0.000000        0.0  0.0875   Yes    True   \n","6  0.2  Alice  0.033333     0.0  0.033333        0.2  0.1000    No   False   \n","\n","   Location  High_Income  \n","0       0.0        False  \n","3       0.0         True  \n","4       0.2         True  \n","5       0.0        False  \n","6       0.2        False  \n","\n","Concatenated DataFrame:\n","    ID   Name       Age  Gender    Income  Education  Score Label Has_Car  \\\n","0  1.0   John  0.000000     1.0  0.000000        0.0  0.875   Yes    True   \n","1  4.0   Mary  1.000000     0.0  0.666667        3.0  0.000   Yes    True   \n","2  5.0   Jane  0.666667     0.0  1.000000        0.0  0.750    No    True   \n","3  1.0   John  0.000000     1.0  0.000000        0.0  0.875   Yes    True   \n","4  2.0  Alice  0.333333     0.0  0.333333        2.0  1.000    No   False   \n","\n","   Location  High_Income  \n","0       0.0        False  \n","1       0.0         True  \n","2       2.0         True  \n","3       0.0        False  \n","4       2.0        False  \n","Augmented DataFrame with SMOTE:\n","    ID       Age  Gender    Income  Education  Score  Location  Name_Alice  \\\n","0  1.0  0.000000     1.0  0.000000        0.0  0.875       0.0       False   \n","1  4.0  1.000000     0.0  0.666667        3.0  0.000       0.0       False   \n","2  5.0  0.666667     0.0  1.000000        0.0  0.750       2.0       False   \n","3  1.0  0.000000     1.0  0.000000        0.0  0.875       0.0       False   \n","4  2.0  0.333333     0.0  0.333333        2.0  1.000       2.0        True   \n","\n","   Name_Jane  Name_John  Name_Mary  Label_No  Label_Yes  Has_Car_False  \\\n","0      False       True      False     False       True          False   \n","1      False      False       True     False       True          False   \n","2       True      False      False      True      False          False   \n","3      False       True      False     False       True          False   \n","4      False      False      False      True      False           True   \n","\n","   Has_Car_True  High_Income  \n","0          True        False  \n","1          True         True  \n","2          True         True  \n","3          True        False  \n","4         False        False  \n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.base import BaseEstimator, TransformerMixin\n","\n","# Original data\n","data = {\n","    'ID': [1, 2, 3, 4, 5],\n","    'Name': ['John', 'Alice', 'Bob', 'Mary', 'Jane'],\n","    'Age': [25, 30, np.nan, 40, 35],\n","    'Gender': ['M', 'F', 'M', 'F', 'F'],\n","    'Income': [50000, 60000, 45000, 70000, 80000],\n","    'Education': ['Bachelor', 'Master', 'High School', 'PhD', 'Bachelor'],\n","    'Score': [85, 90, 75, 95, 80],\n","    'Label': ['Yes', 'No', 'Yes', 'Yes', 'No'],\n","    'Has_Car': [True, False, True, True, True],\n","    'Location': ['City', 'Suburb', 'Rural', 'City', 'Suburb'],\n","    'High_Income': [False, False, True, True, True]  # Target feature\n","}\n","\n","# Convert data to DataFrame\n","df = pd.DataFrame(data)\n","\n","# Define numerical and categorical columns\n","numerical_cols = ['Age', 'Income', 'Score']\n","categorical_cols = ['Gender', 'Education', 'Label', 'Has_Car', 'Location']\n","\n","# Custom transformer to remove outliers using IQR method\n","class RemoveOutliers(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        pass\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        Q1 = np.percentile(X, 25, axis=0)\n","        Q3 = np.percentile(X, 75, axis=0)\n","        IQR = Q3 - Q1\n","        return X[~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)]\n","\n","# Define preprocessing steps for numerical and categorical columns\n","numerical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with mean\n","    ('scaler', MinMaxScaler())  # Scale features to a range between 0 and 1\n","])\n","\n","categorical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with mode\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical features\n","])\n","\n","# Apply transformations to numerical and categorical columns\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numerical_transformer, numerical_cols),\n","        ('cat', categorical_transformer, categorical_cols)\n","    ])\n","\n","# Pipeline for preprocessing and removing outliers\n","pipeline = Pipeline(steps=[\n","    ('preprocessor', preprocessor),\n","    ('outlier_remover', RemoveOutliers())\n","])\n","\n","# Apply preprocessing pipeline to the dataframe\n","df_preprocessed = pipeline.fit_transform(df)\n","\n","# Convert preprocessed data back to DataFrame\n","df_preprocessed = pd.DataFrame(df_preprocessed)\n","\n","print(\"Preprocessed DataFrame:\")\n","print(df_preprocessed)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z9dxFOpALcbi","executionInfo":{"status":"ok","timestamp":1712554266624,"user_tz":0,"elapsed":428,"user":{"displayName":"Márk Németh","userId":"08804061133513418249"}},"outputId":"cbf21a2f-9dfa-4c28-a535-67294b168478"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Preprocessed DataFrame:\n","         0         1     2    3    4    5    6    7    8    9    10   11   12  \\\n","0  0.000000  0.142857  0.50  0.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  1.0   \n","1  0.666667  1.000000  0.25  1.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0   \n","\n","    13   14   15  \n","0  1.0  0.0  0.0  \n","1  0.0  0.0  1.0  \n"]}]}]}