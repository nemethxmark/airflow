{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOM/BF5HHBciHgQEzLDb2pV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0xDP7j_8rDku","outputId":"54c4af7c-a568-4bf0-ece1-07ccbd914b87"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data...\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 0s 0us/step\n","25000 train sequences\n","25000 test sequences\n","Pad sequences (samples x time)\n","input_train shape: (25000, 500)\n","input_test shape: (25000, 500)\n","Training...\n","Epoch 1/10\n","625/625 [==============================] - 79s 123ms/step - loss: 0.6094 - accuracy: 0.6510 - val_loss: 0.5583 - val_accuracy: 0.7068\n","Epoch 2/10\n","625/625 [==============================] - 79s 127ms/step - loss: 0.3680 - accuracy: 0.8406 - val_loss: 0.3798 - val_accuracy: 0.8478\n","Epoch 3/10\n","625/625 [==============================] - 75s 119ms/step - loss: 0.2654 - accuracy: 0.8942 - val_loss: 0.4132 - val_accuracy: 0.8400\n","Epoch 4/10\n","331/625 [==============>...............] - ETA: 32s - loss: 0.1690 - accuracy: 0.9373"]}],"source":["\"\"\"\n","Recurrent Neural Network (RNN) Example:\n","\n","Description:\n","    This example demonstrates the creation and training of a Recurrent Neural Network (RNN) using TensorFlow and Keras. RNNs are a type of neural network architecture designed for sequence data, making them suitable for tasks such as time series prediction, natural language processing, and speech recognition. In this example, we'll use an RNN to perform sentiment analysis on movie reviews from the IMDB dataset.\n","\n","Alternatives:\n","    - Long Short-Term Memory (LSTM) networks: A type of RNN with improved memory capabilities, making them better suited for capturing long-term dependencies in sequences.\n","    - Gated Recurrent Unit (GRU) networks: Similar to LSTMs, but with a simplified architecture, making them more computationally efficient.\n","\n","Benefits:\n","    - Suitable for sequence data: RNNs are designed to handle sequential data such as text, time series, and audio.\n","    - Captures temporal dependencies: RNNs can learn patterns and relationships across different time steps in a sequence.\n","    - Flexibility: RNNs can be applied to various tasks, including sequence prediction, classification, and generation.\n","\n","Downsides:\n","    - Vanishing gradients: RNNs are prone to the vanishing gradient problem, making it difficult to capture long-term dependencies in sequences.\n","    - Computationally intensive: Training RNNs can be computationally expensive, especially with large datasets and complex architectures.\n","    - Memory constraints: RNNs may struggle with long sequences due to memory limitations, leading to truncated sequences or inefficient training.\n","\n","\"\"\"\n","\n","import tensorflow as tf\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n","\n","# Load the IMDB dataset\n","max_features = 10000  # Consider only the top 10,000 most frequent words\n","maxlen = 500  # Limit the review length to 500 words\n","batch_size = 32\n","\n","print('Loading data...')\n","(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n","print(len(input_train), 'train sequences')\n","print(len(input_test), 'test sequences')\n","\n","# Pad sequences to a fixed length\n","print('Pad sequences (samples x time)')\n","input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n","input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n","print('input_train shape:', input_train.shape)\n","print('input_test shape:', input_test.shape)\n","\n","# Define the RNN model\n","model = Sequential()\n","model.add(Embedding(max_features, 32))\n","model.add(SimpleRNN(32))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","print('Training...')\n","model.fit(input_train, y_train, epochs=10, batch_size=batch_size, validation_split=0.2)\n","\n","# Evaluate the model\n","print('Evaluation...')\n","test_loss, test_acc = model.evaluate(input_test, y_test)\n","print('Test accuracy:', test_acc)\n","\n","\"\"\"\n","What the Model is Trying to Learn:\n","\n","The model in this example is attempting to learn sentiment analysis on movie reviews from the IMDB dataset. Specifically, it aims to predict whether a given movie review is positive or negative based on the text content of the review.\n","\n","The IMDB dataset consists of movie reviews labeled as positive or negative. The reviews are represented as sequences of words, and each word is encoded as an integer index. The model processes these sequences of word indices using an Embedding layer to convert them into dense vector representations. Then, it uses a SimpleRNN layer to capture sequential dependencies in the reviews. Finally, a Dense layer with a sigmoid activation function is used to output the predicted sentiment score for each review, with values close to 0 indicating negative sentiment and values close to 1 indicating positive sentiment.\n","\n","During training, the model learns to minimize the binary cross-entropy loss between the predicted sentiment scores and the true labels (positive or negative) of the reviews. Through backpropagation and gradient descent, the model adjusts its parameters to improve its predictions over time.\n","\n","In summary, the model is trained to understand the sentiment expressed in movie reviews and predict whether each review is positive or negative based on its text content.\n","\"\"\"\n","\n","\n"]},{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n","from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n","from sklearn.model_selection import GridSearchCV\n","\n","# Load the IMDB dataset\n","max_features = 10000  # Consider only the top 10,000 most frequent words\n","maxlen = 500  # Limit the review length to 500 words\n","batch_size = 32\n","\n","print('Loading data...')\n","(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n","print(len(input_train), 'train sequences')\n","print(len(input_test), 'test sequences')\n","\n","# Pad sequences to a fixed length\n","print('Pad sequences (samples x time)')\n","input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n","input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n","print('input_train shape:', input_train.shape)\n","print('input_test shape:', input_test.shape)\n","\n","# Define the RNN model\n","def create_rnn_model(optimizer='adam', units=32):\n","    model = Sequential()\n","    model.add(Embedding(max_features, 32))\n","    model.add(SimpleRNN(units))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","# Create KerasClassifier for grid search\n","model = KerasClassifier(build_fn=create_rnn_model, epochs=10, batch_size=batch_size, verbose=0)\n","\n","# Define the grid search parameters\n","param_grid = {\n","    'optimizer': ['adam', 'rmsprop', 'sgd'],\n","    'units': [32, 64, 128],\n","}\n","\n","# Perform grid search\n","grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n","grid_result = grid.fit(input_train, y_train)\n","\n","# Summarize results\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","means = grid_result.cv_results_['mean_test_score']\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","for mean, stdev, param in zip(means, stds, params):\n","    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n","\n","\"\"\"\n","Explanation of Layers in the RNN Model:\n","\n","The RNN (Recurrent Neural Network) model in this example consists of several layers, each serving a specific purpose in the architecture. Here's an explanation of each layer:\n","\n","1. Embedding Layer:\n","   - Purpose: The Embedding layer converts integer-encoded words into dense vector representations. Each word is represented by a vector of fixed size, where similar words have similar vector representations.\n","   - Practices: The size of the embedding dimension (e.g., 32 in this example) is a hyperparameter that can be tuned based on the vocabulary size and complexity of the task. It's common to use pre-trained word embeddings for better performance, especially in NLP tasks with large vocabularies.\n","   - Rule of Thumb: Choose the embedding dimension large enough to capture semantic relationships between words but small enough to prevent overfitting. Pre-trained embeddings such as Word2Vec, GloVe, or FastText can provide useful initializations.\n","\n","2. SimpleRNN Layer:\n","   - Purpose: The SimpleRNN (Simple Recurrent Neural Network) layer processes sequential data by maintaining a hidden state that captures information from previous time steps. It applies a simple transformation to the input at each time step and passes the hidden state to the next time step.\n","   - Practices: The number of units in the SimpleRNN layer (e.g., 32 in this example) determines the dimensionality of the hidden state and the capacity of the model to capture temporal dependencies. Increasing the number of units can enhance the model's ability to learn complex patterns but may also increase computational complexity and risk overfitting.\n","   - Rule of Thumb: Start with a moderate number of units and adjust based on the complexity of the data and the task. Consider using more advanced recurrent units such as LSTM or GRU if the SimpleRNN struggles to capture long-term dependencies.\n","\n","3. Dense Layer:\n","   - Purpose: The Dense layer is a fully connected layer that transforms the output of the RNN layer into a single output value. In this binary classification task, a sigmoid activation function is used to produce a probability score indicating the sentiment (positive or negative) of the input sequence.\n","   - Practices: The number of units in the Dense layer is typically small, as it only needs to map the features learned by the preceding layers to the final output. The activation function (sigmoid in this case) ensures that the output is in the range [0, 1], representing the probability of the positive class.\n","   - Rule of Thumb: Use the sigmoid activation function for binary classification tasks. For multi-class classification tasks, consider using softmax activation with multiple output units corresponding to each class.\n","\n","These layers collectively form the RNN model, which is trained to learn the sentiment expressed in movie reviews and predict whether each review is positive or negative based on its text content.\n","\"\"\"\n"],"metadata":{"id":"98kwYiBUsWp9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","\n","# Load the IMDB dataset\n","max_features = 10000\n","maxlen = 500\n","print('Loading data...')\n","(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n","print(len(input_train), 'train sequences')\n","print(len(input_test), 'test sequences')\n","\n","# Pad sequences to fixed length\n","input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n","input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n","\n","# Load pre-trained RNN model\n","pretrained_model = load_model('pretrained_rnn_model.h5')\n","\n","# Freeze pre-trained layers\n","for layer in pretrained_model.layers:\n","    layer.trainable = False\n","\n","# Add new top layers for fine-tuning\n","fine_tuned_model = Sequential()\n","fine_tuned_model.add(pretrained_model)\n","fine_tuned_model.add(Dense(128, activation='relu'))\n","fine_tuned_model.add(Dropout(0.5))\n","fine_tuned_model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the fine-tuned model\n","fine_tuned_model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Fine-tune the model on the IMDB dataset\n","fine_tuned_model.fit(input_train, y_train, batch_size=32, epochs=5, validation_data=(input_test, y_test))\n"],"metadata":{"id":"jKF89br5s1Gb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cOe2ID6kr43G"},"execution_count":null,"outputs":[]}]}